{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lpips in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from lpips) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from lpips) (0.18.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from lpips) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from lpips) (1.14.0)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from lpips) (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torch>=0.4.0->lpips) (2021.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from torchvision>=0.2.1->lpips) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.0->lpips) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.0->lpips) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sc23gd\\.conda\\envs\\vptr\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\.conda\\envs\\VPTR\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Test the trained VPTR\n",
    "\n",
    "# %%\n",
    "!pip install lpips\n",
    "!pip install matplotlib\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from model import VPTREnc, VPTRDec, VPTRDisc, init_weights, VPTRFormerFAR, VPTRFormerNAR\n",
    "from model import GDL, MSELoss, L1Loss, GANLoss, BiPatchNCE\n",
    "from utils import VidCenterCrop, VidPad, VidResize, VidNormalize, VidReNormalize, VidCrop, VidRandomHorizontalFlip, VidRandomVerticalFlip, VidToTensor\n",
    "from utils import visualize_batch_clips, save_ckpt, load_ckpt, set_seed, AverageMeters, init_loss_dict, write_summary, resume_training, write_code_files\n",
    "from utils import set_seed, PSNR, SSIM, MSEScore, get_dataloader\n",
    "import lpips\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'loss_dict', 'Module_state_dict', 'optimizer_state_dict', 'code'])\n",
      "'modules' key not found in the checkpoint\n",
      "'optimizers' key not found in the checkpoint\n",
      "Checkpoint contains epoch information: True\n",
      "Checkpoint contains history information: False\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load the checkpoint file and inspect its contents\n",
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('C:\\VPTR_jigsaws\\jigsaws_suturing\\VPTR_ckpts\\JIGSAWS_ResnetAE_MSEGDLgan_ckpt\\epoch_100.tar', map_location=torch.device('cpu'))\n",
    "\n",
    "# Print out the keys in the checkpoint\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "\n",
    "# Check if 'modules' key is present and print its keys\n",
    "if 'modules' in checkpoint:\n",
    "    print(\"Modules keys:\", checkpoint['modules'].keys())\n",
    "else:\n",
    "    print(\"'modules' key not found in the checkpoint\")\n",
    "\n",
    "# Print the optimizer keys if present\n",
    "if 'optimizers' in checkpoint:\n",
    "    print(\"Optimizers keys:\", checkpoint['optimizers'].keys())\n",
    "else:\n",
    "    print(\"'optimizers' key not found in the checkpoint\")\n",
    "\n",
    "# Print additional information if needed\n",
    "print(\"Checkpoint contains epoch information:\", 'epoch' in checkpoint)\n",
    "print(\"Checkpoint contains history information:\", 'history' in checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "resume_ckpt = Path('C:\\VPTR_jigsaws\\jigsaws_suturing\\VPTR_ckpts\\JIGSAWS_FAR_ckpt\\epoch_200.tar') #The trained Transformer checkpoint file\n",
    "resume_AE_ckpt = Path('C:\\VPTR_jigsaws\\jigsaws_suturing\\VPTR_ckpts\\JIGSAWS_ResnetAE_MSEGDLgan_ckpt\\epoch_100.tar') #The trained AutoEncoder checkpoint file\n",
    "num_past_frames = 10\n",
    "num_future_frames = 20\n",
    "encH, encW, encC = 8, 8, 528\n",
    "TSLMA_flag = False\n",
    "rpe = True\n",
    "model_flag = 'FAR' #'NAR' for NAR model, 'FAR' for FAR model\n",
    "\n",
    "img_channels = 3 # 1 for KTH and MovingMNIST, 3 for BAIR\n",
    "N = 2\n",
    "device = torch.device('cuda:0')\n",
    "loss_name_list = ['T_MSE', 'T_GDL', 'T_gan', 'T_total', 'Dtotal', 'Dfake', 'Dreal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\.conda\\envs\\VPTR\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#Set the padding_type to be \"zero\" for BAIR dataset\n",
    "VPTR_Enc = VPTREnc(img_channels, feat_dim = encC, n_downsampling = 3, padding_type = 'reflect').to(device) \n",
    "\n",
    "#Set the padding_type to be \"zero\" for BAIR dataset, set the out_layer to be 'Sigmoid' for MovingMNIST\n",
    "VPTR_Dec = VPTRDec(img_channels, feat_dim = encC, n_downsampling = 3, out_layer = 'Tanh', padding_type = 'reflect').to(device) \n",
    "VPTR_Enc = VPTR_Enc.eval()\n",
    "VPTR_Dec = VPTR_Dec.eval()\n",
    "\n",
    "if model_flag == 'NAR':\n",
    "    VPTR_Transformer = VPTRFormerNAR(num_past_frames, num_future_frames, encH=encH, encW = encW, d_model=encC, \n",
    "                                         nhead=8, num_encoder_layers=4, num_decoder_layers=8, dropout=0.1, \n",
    "                                         window_size=4, Spatial_FFN_hidden_ratio=4, TSLMA_flag = TSLMA_flag, rpe=rpe).to(device)\n",
    "else:\n",
    "    VPTR_Transformer = VPTRFormerFAR(num_past_frames, num_future_frames, encH=encH, encW = encW, d_model=encC, \n",
    "                                    nhead=8, num_encoder_layers=12, dropout=0.1, \n",
    "                                    window_size=4, Spatial_FFN_hidden_ratio=4, rpe=rpe).to(device)\n",
    "\n",
    "VPTR_Transformer = VPTR_Transformer.eval()\n",
    "\n",
    "#load the trained autoencoder, we initialize the discriminator from scratch, for a balanced training\n",
    "loss_dict, start_epoch = resume_training({'VPTR_Enc': VPTR_Enc, 'VPTR_Dec': VPTR_Dec}, {}, resume_AE_ckpt, loss_name_list)\n",
    "if resume_ckpt is not None:\n",
    "    loss_dict, start_epoch = resume_training({'VPTR_Transformer': VPTR_Transformer}, \n",
    "                                             {}, resume_ckpt, loss_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train_loader, test_loader, renorm_transform = get_dataloader('Suturing', N, 'C:\\\\VPTR_jigsaws\\\\jigsaws_suturing\\\\frames_split', test_past_frames = 20, test_future_frames = 20, ngpus = 1, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_result(pred, fig_name, num_frames, n = 2):\n",
    "    \"\"\"\n",
    "    Plot and save figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, num_frames, figsize = (num_frames, 1))\n",
    "    fig.subplots_adjust(wspace=0., hspace = 0.)\n",
    "\n",
    "    for j in range(num_frames):\n",
    "        ax[j].set_axis_off()\n",
    "        \n",
    "        img = pred[:, j, :, :, :].clone()\n",
    "        img = renorm_transform(img)\n",
    "        img = torch.clamp(img, min = 0., max = 1.)\n",
    "        img = img[n, ...]\n",
    "\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        ax[j].imshow(img, cmap = 'gray')\n",
    "    fig.savefig(f'{fig_name}.pdf', bbox_inches = 'tight')\n",
    "    \n",
    "def FAR_RIL_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device):\n",
    "    \"\"\"\n",
    "    recurrently inference over Latent space, get a worse result\n",
    "    \"\"\"\n",
    "    past_frames, future_frames = sample\n",
    "    past_frames = past_frames.to(device)\n",
    "    future_frames = future_frames.to(device)\n",
    "    assert num_pred == future_frames.shape[1], \"Mismatch between ground truth future frames length and num_pred\"\n",
    "\n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    pred_feats = VPTR_Transformer(past_gt_feats)\n",
    "    \n",
    "    pred_frames = VPTR_Dec(pred_feats[:, -1:, ...])\n",
    "    for i in range(1, num_pred):\n",
    "        if i == 1:\n",
    "            input_feats = torch.cat([past_gt_feats, pred_feats[:, -1:, ...]], dim = 1)\n",
    "        elif i < VPTR_Transformer.num_future_frames:\n",
    "            input_feats = torch.cat([input_feats, pred_future_feat], dim = 1)\n",
    "        else:\n",
    "            input_feats = torch.cat([input_feats, pred_future_feat], dim = 1)\n",
    "            input_feats = input_feats[:, 1:, ...]\n",
    "        \n",
    "        pred_feats = VPTR_Transformer(input_feats)\n",
    "        pred_future_frame = VPTR_Dec(pred_feats[:, -1:, ...])\n",
    "        pred_future_feat = pred_feats[:, -1:, ...]\n",
    "        pred_frames = torch.cat([pred_frames, pred_future_frame], dim = 1)\n",
    "    \n",
    "    return pred_frames, future_frames\n",
    "\n",
    "def FAR_RIP_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device):\n",
    "    \"\"\"\n",
    "    Recursively inference over pixel space.\n",
    "    \"\"\"\n",
    "    past_frames, future_frames = sample\n",
    "    past_frames = past_frames.to(device)\n",
    "    future_frames = future_frames.to(device)\n",
    "    assert num_pred == future_frames.shape[1], \"Mismatch between ground truth future frames length and num_pred\"\n",
    "\n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    pred_feats = VPTR_Transformer(past_gt_feats)\n",
    "\n",
    "    pred_frames = VPTR_Dec(pred_feats[:, -1:, ...])\n",
    "    for i in range(1, num_pred):\n",
    "        pred_future_frame = VPTR_Dec(pred_feats[:, -1:, ...])  # Decode the last predicted feature\n",
    "        pred_future_feat = VPTR_Enc(pred_future_frame)  # Encode the predicted frame\n",
    "\n",
    "        if i == 1:\n",
    "            input_feats = torch.cat([past_gt_feats, pred_future_feat], dim=1)  # Concatenate for the first prediction\n",
    "        else:\n",
    "            input_feats = torch.cat([input_feats, pred_future_feat], dim=1)  # Concatenate with previous input features\n",
    "            input_feats = input_feats[:, 1:, ...]  # Remove the oldest frame from input_feats\n",
    "\n",
    "        pred_feats = VPTR_Transformer(input_feats)\n",
    "        pred_frames = torch.cat([pred_frames, pred_future_frame], dim=1)\n",
    "\n",
    "    return pred_frames, future_frames\n",
    "\n",
    "def NAR_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device):\n",
    "    \"\"\"\n",
    "    NAR model inference function, for the case num_pred is divisible for the num_future_frames of training. e.g. num_pred = 20, num_future_frames = 10\n",
    "    \"\"\"\n",
    "    past_frames, future_frames = sample\n",
    "    past_frames = past_frames.to(device)\n",
    "    future_frames = future_frames.to(device)\n",
    "    assert num_pred == future_frames.shape[1], \"Mismatch between ground truth future frames length and num_pred\"\n",
    "    assert num_pred % VPTR_Transformer.num_future_frames == 0, \"Mismatch of num_pred and trained Transformer\"\n",
    "    \n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    \n",
    "    for i in range(0, num_pred//VPTR_Transformer.num_future_frames):\n",
    "        pred_future_feats = VPTR_Transformer(past_gt_feats)\n",
    "        if i == 0:\n",
    "            pred_frames = VPTR_Dec(pred_future_feats)\n",
    "        else:\n",
    "            pred_frames = torch.cat([pred_frames, VPTR_Dec(pred_future_feats)], dim = 1)\n",
    "        past_gt_feats = pred_future_feats\n",
    "        \n",
    "    \n",
    "    return pred_frames, future_frames\n",
    "\n",
    "\n",
    "def NAR_BAIR_2_to_28_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device):\n",
    "    \"\"\"\n",
    "    Specifically for BAIR dataset, 2 -> 28 prediction.\n",
    "    \"\"\"\n",
    "    past_frames, future_frames = sample\n",
    "    past_frames = past_frames.to(device)\n",
    "    future_frames = future_frames.to(device)\n",
    "    assert num_pred == future_frames.shape[1], \"Mismatch between ground truth future frames length and num_pred\"\n",
    "    #assert num_pred % VPTR_Transformer.num_future_frames == 0, \"Mismatch of num_pred and trained Transformer\"\n",
    "    \n",
    "    pred = []\n",
    "    #prediction 1\n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    pred_future_feats = VPTR_Transformer(past_gt_feats)\n",
    "    pred_frames = VPTR_Dec(pred_future_feats)\n",
    "    pred.append(pred_frames)\n",
    "    #prediction 2\n",
    "    past_frames = pred_frames[:, -2:, ...]\n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    pred_future_feats = VPTR_Transformer(past_gt_feats)\n",
    "    pred_frames = VPTR_Dec(pred_future_feats)\n",
    "    pred.append(pred_frames)\n",
    "    \n",
    "    #prediction 3\n",
    "    past_frames = pred_frames[:, -2:, ...]\n",
    "    past_gt_feats = VPTR_Enc(past_frames)\n",
    "    pred_future_feats = VPTR_Transformer(past_gt_feats)\n",
    "    pred_frames = VPTR_Dec(pred_future_feats)\n",
    "    pred.append(pred_frames[:, 0:-2, ...])\n",
    "    pred_frames = torch.cat(pred, dim = 1)\n",
    "    \n",
    "    return pred_frames, future_frames         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_feat_shape: torch.Size([2, 20, 528, 16, 16])\n",
      "temporal_pos_embed_shape: torch.Size([30, 528])\n",
      "Input x shape: torch.Size([2, 20, 16, 16, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([20, 528])\n",
      "Resized x shape: torch.Size([2, 20, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([20, 128, 528])\n",
      "attn_mask shape: torch.Size([20, 20])\n",
      "x1: torch.Size([20, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([20, 1, 528])\n",
      "Output x shape: torch.Size([2, 20, 8, 8, 528])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m         pred_frames, gt_frames \u001b[38;5;241m=\u001b[39m \u001b[43mFAR_RIP_test_single_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVPTR_Enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVPTR_Dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVPTR_Transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNAR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     10\u001b[0m         pred_frames, gt_frames \u001b[38;5;241m=\u001b[39m NAR_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device)\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36mFAR_RIP_test_single_iter\u001b[1;34m(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device)\u001b[0m\n\u001b[0;32m     64\u001b[0m pred_future_feat \u001b[38;5;241m=\u001b[39m VPTR_Enc(pred_future_frame)  \u001b[38;5;66;03m# Encode the predicted frame\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m     input_feats \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_gt_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_future_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Concatenate for the first prediction\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     input_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_feats, pred_future_feat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate with previous input features\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Example usage of inference functions (adjust num_pred as needed)\n",
    "sample = next(iter(test_loader))\n",
    "num_pred = 10 \n",
    "\n",
    "with torch.no_grad():\n",
    "    if model_flag == 'FAR':\n",
    "        pred_frames, gt_frames = FAR_RIP_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device)\n",
    "    elif model_flag == 'NAR':\n",
    "        pred_frames, gt_frames = NAR_test_single_iter(sample, VPTR_Enc, VPTR_Dec, VPTR_Transformer, num_pred, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Visualize predictions\n",
    "plot_model_result(pred_frames, 'pred_frames', num_pred, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_result(gt_frames, 'gt_frames', num_pred, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import PSNR, SSIM\n",
    "import lpips\n",
    "\n",
    "def pred_ave_metrics(model, data_loader, metric_func, renorm_transform, num_future_frames, num_past_frames, device = 'cuda:0', use_lpips = False, gray_scale = True):\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    ave_metric = np.zeros(num_future_frames)\n",
    "    sample_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(data_loader, 0):\n",
    "            past_frames, future_frames = sample\n",
    "            past_frames = past_frames.to(device)\n",
    "            future_frames = future_frames.to(device)\n",
    "            pred = model(past_frames)\n",
    "            for i in range(0, num_future_frames):\n",
    "                pred_t = pred[:, i, ...]\n",
    "                future_frames_t = future_frames[:, i, ...]\n",
    "                \n",
    "                if not use_lpips:\n",
    "                    pred_t = renorm_transform(pred_t)\n",
    "                    future_frames_t = renorm_transform(future_frames_t)\n",
    "                elif use_lpips and gray_scale:\n",
    "                    pred_t = pred_t.repeat(1, 3, 1, 1)\n",
    "                    future_frames_t = future_frames_t.repeat(1, 3, 1, 1)\n",
    "                    \n",
    "                m = metric_func(pred_t, future_frames_t)*pred_t.shape[0]\n",
    "                try:\n",
    "                    ave_metric[i] += m.mean()\n",
    "                except AttributeError:\n",
    "                    ave_metric[i] += m\n",
    "                \n",
    "            sample_num += pred.shape[0]\n",
    "\n",
    "    ave_metric = ave_metric / sample_num\n",
    "    return ave_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\.conda\\envs\\VPTR\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc23gd\\.conda\\envs\\VPTR\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\sc23gd/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:09<00:00, 25.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\sc23gd\\.conda\\envs\\VPTR\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "input_feat_shape: torch.Size([2, 10, 528, 16, 16])\n",
      "temporal_pos_embed_shape: torch.Size([30, 528])\n",
      "Input x shape: torch.Size([2, 10, 16, 16, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "Input x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "local_window_pos_embed shape: torch.Size([4, 4, 528])\n",
      "temporal_pos_embed shape: torch.Size([10, 528])\n",
      "Resized x shape: torch.Size([2, 10, 8, 8, 528])\n",
      "x1 shape after norm3: torch.Size([10, 128, 528])\n",
      "attn_mask shape: torch.Size([10, 10])\n",
      "x1: torch.Size([10, 128, 528])\n",
      "temporal_pos_embed[:T, None, :]: torch.Size([10, 1, 528])\n",
      "Output x shape: torch.Size([2, 10, 8, 8, 528])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(VPTR_Enc, VPTR_Transformer, VPTR_Dec)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m psnr_list \u001b[38;5;241m=\u001b[39m \u001b[43mpred_ave_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPSNR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenorm_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_future_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_past_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lpips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ssim_list \u001b[38;5;241m=\u001b[39m pred_ave_metrics(model, test_loader, ssim, renorm_transform, num_future_frames, num_past_frames, device, use_lpips\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m lpips_list \u001b[38;5;241m=\u001b[39m pred_ave_metrics(model, test_loader, loss_fn_alex, renorm_transform, num_future_frames, num_past_frames, device, use_lpips\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m, in \u001b[0;36mpred_ave_metrics\u001b[1;34m(model, data_loader, metric_func, renorm_transform, num_future_frames, num_past_frames, device, use_lpips, gray_scale)\u001b[0m\n\u001b[0;32m     25\u001b[0m     pred_t \u001b[38;5;241m=\u001b[39m pred_t\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m     future_frames_t \u001b[38;5;241m=\u001b[39m future_frames_t\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_frames_t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mpred_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     ave_metric[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\VPTR_jigsaws\\utils\\metrics.py:25\u001b[0m, in \u001b[0;36mPSNR\u001b[1;34m(x, y, data_range)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(data_range)\n\u001b[0;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(data_range)\n\u001b[1;32m---> 25\u001b[0m mse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     26\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog10(mse \u001b[38;5;241m+\u001b[39m EPS)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(score)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Initialize metric functions\n",
    "loss_fn_alex = lpips.LPIPS(net='alex').to(device)\n",
    "ssim = SSIM()\n",
    "\n",
    "# %%\n",
    "# Create the complete model\n",
    "model = nn.Sequential(VPTR_Enc, VPTR_Transformer, VPTR_Dec)\n",
    "\n",
    "# %%\n",
    "# Calculate metrics\n",
    "psnr_list = pred_ave_metrics(model, test_loader, PSNR, renorm_transform, num_future_frames, num_past_frames, device, use_lpips=False)\n",
    "ssim_list = pred_ave_metrics(model, test_loader, ssim, renorm_transform, num_future_frames, num_past_frames, device, use_lpips=False)\n",
    "lpips_list = pred_ave_metrics(model, test_loader, loss_fn_alex, renorm_transform, num_future_frames, num_past_frames, device, use_lpips=True)\n",
    "\n",
    "# %%\n",
    "# Print the calculated metrics\n",
    "print(\"PSNR:\", psnr_list)\n",
    "print(\"SSIM:\", ssim_list)\n",
    "print(\"LPIPS:\", lpips_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VPTR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
