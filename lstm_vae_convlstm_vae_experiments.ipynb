{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import secrets\n\nimport easydict\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-23T15:26:38.840122Z","iopub.execute_input":"2024-06-23T15:26:38.840540Z","iopub.status.idle":"2024-06-23T15:26:56.380485Z","shell.execute_reply.started":"2024-06-23T15:26:38.840509Z","shell.execute_reply":"2024-06-23T15:26:56.379633Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-23 15:26:45.093793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-23 15:26:45.093903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-23 15:26:45.210478: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size=4096, hidden_size=1024, num_layers=2):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(\n            input_size,\n            hidden_size,\n            num_layers,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n    def forward(self, x):\n        # x: tensor of shape (batch_size, seq_length, hidden_size)\n        outputs, (hidden, cell) = self.lstm(x)\n        return (hidden, cell)\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self, input_size=4096, hidden_size=1024, output_size=4096, num_layers=2\n    ):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(\n            input_size,\n            hidden_size,\n            num_layers,\n            batch_first=True,\n            bidirectional=False,\n        )\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        # x: tensor of shape (batch_size, seq_length, hidden_size)\n        output, (hidden, cell) = self.lstm(x, hidden)\n        prediction = self.fc(output)\n        return prediction, (hidden, cell)\n\n\nclass LSTMVAE(nn.Module):\n    \"\"\"LSTM-based Variational Auto Encoder\"\"\"\n\n    def __init__(\n        self, input_size, hidden_size, latent_size, device=torch.device(\"cuda\")\n    ):\n        \"\"\"\n        input_size: int, batch_size x sequence_length x input_dim\n        hidden_size: int, output size of LSTM AE\n        latent_size: int, latent z-layer size\n        num_lstm_layer: int, number of layers in LSTM\n        \"\"\"\n        super(LSTMVAE, self).__init__()\n        self.device = device\n\n        # dimensions\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n        self.num_layers = 1\n\n        # lstm ae\n        self.lstm_enc = Encoder(\n            input_size=input_size, hidden_size=hidden_size, num_layers=self.num_layers\n        )\n        self.lstm_dec = Decoder(\n            input_size=latent_size,\n            output_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=self.num_layers,\n        )\n\n        self.fc21 = nn.Linear(self.hidden_size, self.latent_size)\n        self.fc22 = nn.Linear(self.hidden_size, self.latent_size)\n        self.fc3 = nn.Linear(self.latent_size, self.hidden_size)\n\n    def reparametize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        noise = torch.randn_like(std).to(self.device)\n\n        z = mu + noise * std\n        return z\n\n    def forward(self, x):\n        batch_size, seq_len, feature_dim = x.shape\n\n        # encode input space to hidden space\n        enc_hidden = self.lstm_enc(x)\n        enc_h = enc_hidden[0].view(self.num_layers, batch_size, self.hidden_size).to(self.device)\n        enc_c = enc_hidden[1].view(self.num_layers, batch_size, self.hidden_size).to(self.device)\n\n        # extract latent variable z(hidden space to latent space)\n        mean = self.fc21(enc_h[-1])\n        logvar = self.fc22(enc_h[-1])\n        z = self.reparametize(mean, logvar)  # batch_size x latent_size\n\n        # initialize hidden state as inputs\n        h_ = self.fc3(z).view(self.num_layers, batch_size, self.hidden_size)\n        c_ = torch.zeros_like(h_)\n        \n        # decode latent space to input space\n        z = z.unsqueeze(1).repeat(1, seq_len, 1)\n        z = z.view(batch_size, seq_len, self.latent_size).to(self.device)\n\n        # initialize hidden state\n        hidden = (h_.contiguous(), c_.contiguous())\n        reconstruct_output, hidden = self.lstm_dec(z, hidden)\n\n        x_hat = reconstruct_output\n\n        # calculate vae loss\n        losses = self.loss_function(x_hat, x, mean, logvar)\n        m_loss, recon_loss, kld_loss = losses[\"loss\"], losses[\"Reconstruction_Loss\"], losses[\"KLD\"]\n\n        return m_loss, x_hat, (recon_loss, kld_loss)\n\n    def loss_function(self, *args, **kwargs) -> dict:\n        \"\"\"\n        Computes the VAE loss function.\n        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n        :param args:\n        :param kwargs:\n        :return:\n        \"\"\"\n        recons = args[0]\n        input = args[1]\n        mu = args[2]\n        log_var = args[3]\n\n        kld_weight = 0.00025  # Account for the minibatch samples from the dataset\n        recons_loss = F.mse_loss(recons, input)\n\n        kld_loss = torch.mean(\n            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n        )\n\n        loss = recons_loss + kld_weight * kld_loss\n        return {\n            \"loss\": loss,\n            \"Reconstruction_Loss\": recons_loss.detach(),\n            \"KLD\": -kld_loss.detach(),\n        }\n\n\nclass LSTMAE(nn.Module):\n    \"\"\"LSTM-based Auto Encoder\"\"\"\n\n    def __init__(self, input_size, hidden_size, latent_size, device=torch.device(\"cuda\")):\n        \"\"\"\n        input_size: int, batch_size x sequence_length x input_dim\n        hidden_size: int, output size of LSTM AE\n        latent_size: int, latent z-layer size\n        num_lstm_layer: int, number of layers in LSTM\n        \"\"\"\n        super(LSTMAE, self).__init__()\n        self.device = device\n\n        # dimensions\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n\n        # lstm ae\n        self.lstm_enc = Encoder(\n            input_size=input_size,\n            hidden_size=hidden_size,\n        )\n        self.lstm_dec = Decoder(\n            input_size=input_size,\n            output_size=input_size,\n            hidden_size=hidden_size,\n        )\n\n        self.criterion = nn.MSELoss()\n\n    def forward(self, x):\n        batch_size, seq_len, feature_dim = x.shape\n\n        enc_hidden = self.lstm_enc(x)\n\n        temp_input = torch.zeros((batch_size, seq_len, feature_dim), dtype=torch.float).to(\n            self.device\n        )\n        hidden = enc_hidden\n        reconstruct_output, hidden = self.lstm_dec(temp_input, hidden)\n        reconstruct_loss = self.criterion(reconstruct_output, x)\n\n        return reconstruct_loss, reconstruct_output, (0, 0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch import nn\n# from torch.nn import functional as F\n\n# class ConvLSTMCell(nn.Module):\n#     def __init__(self, input_channels, hidden_channels, kernel_size):\n#         super(ConvLSTMCell, self).__init__()\n        \n#         self.input_channels = input_channels\n#         self.hidden_channels = hidden_channels\n#         self.kernel_size = kernel_size\n#         self.padding = kernel_size // 2\n        \n#         self.conv = nn.Conv2d(\n#             in_channels=self.input_channels + self.hidden_channels,\n#             out_channels=4 * self.hidden_channels,\n#             kernel_size=self.kernel_size,\n#             padding=self.padding,\n#             bias=True\n#         )\n\n#     def forward(self, x, h, c):\n#         combined = torch.cat([x, h], dim=1)\n#         combined_conv = self.conv(combined)\n#         cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_channels, dim=1)\n#         i = torch.sigmoid(cc_i)\n#         f = torch.sigmoid(cc_f)\n#         o = torch.sigmoid(cc_o)\n#         g = torch.tanh(cc_g)\n        \n#         c_next = f * c + i * g\n#         h_next = o * torch.tanh(c_next)\n        \n#         return h_next, c_next\n\n# class ConvLSTMEncoder(nn.Module):\n#     def __init__(self, input_channels, hidden_channels, kernel_size):\n#         super(ConvLSTMEncoder, self).__init__()\n#         self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n    \n#     def forward(self, x):\n#         batch_size, seq_len, channels, height, width = x.size()\n#         h = torch.zeros(batch_size, self.convlstm.hidden_channels, height, width).to(x.device)\n#         c = torch.zeros(batch_size, self.convlstm.hidden_channels, height, width).to(x.device)\n        \n#         for t in range(seq_len):\n#             h, c = self.convlstm(x[:, t, :, :, :], h, c)\n        \n#         return h, c\n\n# class ConvLSTMDecoder(nn.Module):\n#     def __init__(self, input_channels, hidden_channels, output_channels, kernel_size):\n#         super(ConvLSTMDecoder, self).__init__()\n#         self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n#         self.conv_out = nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1)\n    \n#     def forward(self, x, h, c, seq_len):\n#         outputs = []\n        \n#         for _ in range(seq_len):\n#             h, c = self.convlstm(x, h, c)\n#             output = self.conv_out(h)\n#             outputs.append(output)\n#             x = output\n        \n#         return torch.stack(outputs, dim=1)\n\n# class CONVLSTMVAE(nn.Module):\n#     def __init__(self, input_channels, hidden_channels, latent_size, kernel_size=3):\n#         super(CONVLSTMVAE, self).__init__()\n        \n#         self.encoder = ConvLSTMEncoder(input_channels, hidden_channels, kernel_size)\n#         self.decoder = ConvLSTMDecoder(input_channels, hidden_channels, input_channels, kernel_size)\n        \n#         self.fc_mu = nn.Linear(hidden_channels * 64 * 64, latent_size)  # Assuming 64x64 spatial dimensions\n#         self.fc_logvar = nn.Linear(hidden_channels * 64 * 64, latent_size)\n#         self.fc_decode = nn.Linear(latent_size, hidden_channels * 64 * 64)\n        \n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n    \n#     def forward(self, x):\n#         batch_size, seq_len, channels, height, width = x.size()\n        \n#         # Encode\n#         h, c = self.encoder(x)\n#         h_flat = h.view(batch_size, -1)\n        \n#         # VAE bottleneck\n#         mu = self.fc_mu(h_flat)\n#         logvar = self.fc_logvar(h_flat)\n#         z = self.reparameterize(mu, logvar)\n        \n#         # Decode\n#         h_decoded = self.fc_decode(z).view(batch_size, -1, height, width)\n#         c_decoded = torch.zeros_like(h_decoded)\n#         x_decoded = torch.zeros(batch_size, channels, height, width).to(x.device)\n        \n#         output = self.decoder(x_decoded, h_decoded, c_decoded, seq_len)\n        \n#         return output, mu, logvar\n    \n#     def loss_function(self, recon_x, x, mu, logvar):\n#         BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n#         KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n#         return BCE + KLD\n\n# # Example usage\n# input_channels = 3  # For RGB videos\n# hidden_channels = 64\n# latent_size = 128\n# model = CONVLSTMVAE(input_channels, hidden_channels, latent_size)\n\n# # Assuming input shape: (batch_size, sequence_length, channels, height, width)\n# sample_input = torch.randn(16, 10, 3, 64, 64)\n# output, mu, logvar = model(sample_input)\n\n# print(f\"Input shape: {sample_input.shape}\")\n# print(f\"Output shape: {output.shape}\")\n# print(f\"Mu shape: {mu.shape}\")\n# print(f\"Logvar shape: {logvar.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport codecs\nimport errno\nimport os\nimport os.path\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\n\n\nclass MovingMNIST(data.Dataset):\n    \"\"\"`MovingMNIST <http://www.cs.toronto.edu/~nitish/unsupervised_video/>`_ Dataset.\n\n    Args:\n        root (string): Root directory of dataset where ``processed/training.pt``\n            and  ``processed/test.pt`` exist.\n        train (bool, optional): If True, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        split (int, optional): Train/test split size. Number defines how many samples\n            belong to test set.\n        download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        transform (callable, optional): A function/transform that takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in an PIL\n            image and returns a transformed version. E.g, ``transforms.RandomCrop``\n    \"\"\"\n\n    urls = [\"https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz\"]\n    raw_folder = \"raw\"\n    processed_folder = \"processed\"\n    training_file = \"moving_mnist_train.pt\"\n    test_file = \"moving_mnist_test.pt\"\n\n    def __init__(\n        self,\n        root,\n        train=True,\n        split=1000,\n        transform=None,\n        target_transform=None,\n        download=False,\n    ):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.split = split\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\n                \"Dataset not found.\" + \" You can use download=True to download it\"\n            )\n\n        if self.train:\n            self.train_data = torch.load(\n                os.path.join(self.root, self.processed_folder, self.training_file)\n            )\n        else:\n            self.test_data = torch.load(\n                os.path.join(self.root, self.processed_folder, self.test_file)\n            )\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (seq, target) where sampled sequences are splitted into a seq\n                    and target part\n        \"\"\"\n\n        # need to iterate over time\n        def _transform_time(data):\n            new_data = None\n            for i in range(data.size(0)):\n                img = Image.fromarray(data[i].numpy(), mode=\"L\")\n                new_data = (\n                    self.transform(img)\n                    if new_data is None\n                    else torch.cat([self.transform(img), new_data], dim=0)\n                )\n            return new_data\n\n        if self.train:\n            seq, target = self.train_data[index, :10], self.train_data[index, 10:]\n        else:\n            seq, target = self.test_data[index, :10], self.test_data[index, 10:]\n\n        if self.transform is not None:\n            seq = _transform_time(seq)\n        if self.target_transform is not None:\n            target = _transform_time(target)\n\n        return seq, target\n\n    def __len__(self):\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n\n    def _check_exists(self):\n        return os.path.exists(\n            os.path.join(self.root, self.processed_folder, self.training_file)\n        ) and os.path.exists(\n            os.path.join(self.root, self.processed_folder, self.test_file)\n        )\n\n    def download(self):\n        \"\"\"Download the Moving MNIST data if it doesn't exist in processed_folder already.\"\"\"\n        import gzip\n\n        from six.moves import urllib\n\n        if self._check_exists():\n            return\n\n        # download files\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        for url in self.urls:\n            print(\"Downloading \" + url)\n            data = urllib.request.urlopen(url)\n            filename = url.rpartition(\"/\")[2]\n            file_path = os.path.join(self.root, self.raw_folder, filename)\n            with open(file_path, \"wb\") as f:\n                f.write(data.read())\n            with open(file_path.replace(\".gz\", \"\"), \"wb\") as out_f, gzip.GzipFile(\n                file_path\n            ) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print(\"Processing...\")\n\n        training_set = torch.from_numpy(\n            np.load(\n                os.path.join(self.root, self.raw_folder, \"mnist_test_seq.npy\")\n            ).swapaxes(0, 1)[: -self.split]\n        )\n        test_set = torch.from_numpy(\n            np.load(\n                os.path.join(self.root, self.raw_folder, \"mnist_test_seq.npy\")\n            ).swapaxes(0, 1)[-self.split :]\n        )\n\n        with open(\n            os.path.join(self.root, self.processed_folder, self.training_file), \"wb\"\n        ) as f:\n            torch.save(training_set, f)\n        with open(\n            os.path.join(self.root, self.processed_folder, self.test_file), \"wb\"\n        ) as f:\n            torch.save(test_set, f)\n\n        print(\"Done!\")\n\n    def __repr__(self):\n        fmt_str = \"Dataset \" + self.__class__.__name__ + \"\\n\"\n        fmt_str += \"    Number of datapoints: {}\\n\".format(self.__len__())\n        tmp = \"train\" if self.train is True else \"test\"\n        fmt_str += \"    Train/test: {}\\n\".format(tmp)\n        fmt_str += \"    Root Location: {}\\n\".format(self.root)\n        tmp = \"    Transforms (if any): \"\n        fmt_str += \"{0}{1}\\n\".format(\n            tmp, self.transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n        )\n        tmp = \"    Target Transforms (if any): \"\n        fmt_str += \"{0}{1}\".format(\n            tmp, self.target_transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n        )\n        return fmt_str\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport secrets\n\nimport easydict\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom models_moving_mnist import LSTMAE, LSTMVAE\n\nwriter = SummaryWriter()\n\n## visualization\ndef imshow(past_data, title=\"MovingMNIST\"):\n    num_img = len(past_data)\n    fig = plt.figure(figsize=(4 * num_img, 4))\n\n    for idx in range(1, num_img + 1):\n        ax = fig.add_subplot(1, num_img + 1, idx)\n        ax.imshow(past_data[idx - 1])\n    plt.suptitle(title, fontsize=30)\n    plt.savefig(f\"{title}\")\n    plt.close()\n\ndef visualize_reconstructions(model, test_loader, device, epoch):\n    model.eval()\n    with torch.no_grad():\n        for i, batch_data in enumerate(test_loader):\n            future_data, past_data = batch_data\n            batch_size = past_data.size(0)\n            example_size = past_data.size(1)\n            image_size = past_data.size(2), past_data.size(3)\n            past_data = past_data.view(batch_size, example_size, -1).float().to(device)\n            \n            _, recon_x, _ = model(past_data)\n            \n            if i == 0:\n                n_examples = min(10, batch_size)\n                examples = past_data[:n_examples].cpu().view(n_examples, example_size, image_size[0], -1)\n                recon_examples = recon_x[:n_examples].cpu().view(n_examples, example_size, image_size[0], -1)\n\n                fig, axes = plt.subplots(2, n_examples, figsize=(20, 4))\n                for j in range(n_examples):\n                    axes[0, j].imshow(examples[j, 0], cmap='gray')\n                    axes[0, j].axis('off')\n                    axes[1, j].imshow(recon_examples[j, 0], cmap='gray')\n                    axes[1, j].axis('off')\n                plt.suptitle(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n                plt.savefig(f\"reconstruction_epoch_{epoch}.png\")\n                plt.close()\n                break\n\ndef train(args, model, train_loader, test_loader):\n    # optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n\n    ## interation setup\n    epochs = tqdm(range(args.max_iter // len(train_loader) + 1))\n\n    ## training\n    count = 0\n    for epoch in epochs:\n        model.train()\n        optimizer.zero_grad()\n        train_iterator = tqdm(\n            enumerate(train_loader), total=len(train_loader), desc=\"training\"\n        )\n\n        for i, batch_data in train_iterator:\n\n            if count > args.max_iter:\n                return model\n            count += 1\n\n            future_data, past_data = batch_data\n\n            ## reshape\n            batch_size = past_data.size(0)\n            example_size = past_data.size(1)\n            image_size = past_data.size(2), past_data.size(3)\n            past_data = (\n                past_data.view(batch_size, example_size, -1).float().to(args.device)\n            )\n\n            mloss, recon_x, info = model(past_data)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            mloss.mean().backward()\n            optimizer.step()\n\n            train_iterator.set_postfix({\"train_loss\": float(mloss.mean())})\n        writer.add_scalar(\"train_loss\", float(mloss.mean()), epoch)\n\n        model.eval()\n        eval_loss = 0\n        test_iterator = tqdm(\n            enumerate(test_loader), total=len(test_loader), desc=\"testing\"\n        )\n\n        with torch.no_grad():\n            for i, batch_data in test_iterator:\n                future_data, past_data = batch_data\n\n                ## reshape\n                batch_size = past_data.size(0)\n                example_size = past_data.size(1)\n                past_data = (\n                    past_data.view(batch_size, example_size, -1).float().to(args.device)\n                )\n\n                mloss, recon_x, info = model(past_data)\n\n                eval_loss += mloss.mean().item()\n\n                test_iterator.set_postfix({\"eval_loss\": float(mloss.mean())})\n\n        eval_loss = eval_loss / len(test_loader)\n        writer.add_scalar(\"eval_loss\", float(eval_loss), epoch)\n        print(\"Evaluation Score : [{}]\".format(eval_loss))\n\n        # Visualize reconstructions every 10 epochs\n        if epoch % 10 == 0:\n            visualize_reconstructions(model, test_loader, args.device, epoch)\n\n    return model\n\n\nif __name__ == \"__main__\":\n\n    # training dataset\n    train_set = MovingMNIST(\n        root=\".data/mnist\",\n        train=True,\n        download=True,\n        transform=transforms.ToTensor(),\n        target_transform=transforms.ToTensor(),\n    )\n\n    # test dataset\n    test_set = MovingMNIST(\n        root=\".data/mnist\",\n        train=False,\n        download=True,\n        transform=transforms.ToTensor(),\n        target_transform=transforms.ToTensor(),\n    )\n\n    args = easydict.EasyDict(\n        {\n            \"batch_size\": 512,\n            \"device\": torch.device(\"cuda\")\n            if torch.cuda.is_available()\n            else torch.device(\"cpu\"),\n            \"input_size\": 4096,\n            \"hidden_size\": 2048,\n            \"latent_size\": 1024,\n            \"learning_rate\": 0.001,\n            \"max_iter\": 1000,\n        }\n    )\n\n    batch_size = args.batch_size\n    input_size = args.input_size\n    hidden_size = args.hidden_size\n    latent_size = args.latent_size\n\n    # define LSTM-based VAE model\n    model = LSTMVAE(input_size, hidden_size, latent_size, device=args.device)\n    model.to(args.device)\n\n    # convert to format of data loader\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_set, batch_size=args.batch_size, shuffle=True\n    )\n    test_loader = torch.utils.data.DataLoader(\n        dataset=test_set, batch_size=args.batch_size, shuffle=False\n    )\n\n    # training\n    trained_model = train(args, model, train_loader, test_loader)\n\n    # save model\n    id_ = secrets.token_hex(nbytes=4)\n    torch.save(trained_model.state_dict(), f\"lstmvae{id_}.model\")\n\n    # load model\n    model_to_load = LSTMVAE(input_size, hidden_size, latent_size, device=args.device)\n    model_to_load.to(args.device)\n    model_to_load.load_state_dict(torch.load(f\"lstmvae{id_}.model\"))\n    model_to_load.eval()\n\n    # show results\n    ## past_data, future_data -> shape: (10,10)\n    future_data, past_data = train_set[0]\n\n    ## reshape\n    example_size = past_data.size(0)\n    image_size = past_data.size(1), past_data.size(2)\n    past_data = past_data.view(example_size, -1).float().to(args.device)\n    _, recon_data, info = model_to_load(past_data.unsqueeze(0))\n\n    nhw_orig = past_data.view(example_size, image_size[0], -1).cpu()\n    nhw_recon = (\n        recon_data.squeeze(0)\n        .view(example_size, image_size[0], -1)\n        .detach()\n        .cpu()\n        .numpy()\n    )\n\n    imshow(nhw_orig, title=f\"final_input{id_}\")\n    imshow(nhw_recon, title=f\"final_output{id_}\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}