{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30628,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThe\n[Convolutional LSTM](https://papers.nips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf)\narchitectures bring together time series processing and computer vision by\nintroducing a convolutional recurrent cell in a LSTM layer. In this example, we will explore the\nConvolutional LSTM model in an application to next-frame prediction, the process\nof predicting what video frames come next given a series of past frames.","metadata":{"id":"wZDPozjbLyWn"}},{"cell_type":"markdown","source":"Setup","metadata":{"id":"9kISI-uXL6kp"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorboard\n!pip install --upgrade wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:34:54.883462Z","iopub.execute_input":"2024-05-16T13:34:54.884248Z","iopub.status.idle":"2024-05-16T13:35:25.888901Z","shell.execute_reply.started":"2024-05-16T13:34:54.884211Z","shell.execute_reply":"2024-05-16T13:35:25.887710Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\nSuccessfully installed wandb-0.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import TensorBoard\nimport wandb\nfrom wandb.keras import WandbCallback","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:35:25.890793Z","iopub.execute_input":"2024-05-16T13:35:25.891619Z","iopub.status.idle":"2024-05-16T13:35:27.121167Z","shell.execute_reply.started":"2024-05-16T13:35:25.891575Z","shell.execute_reply":"2024-05-16T13:35:27.119280Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorBoard\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbCallback\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb.keras'"],"ename":"ModuleNotFoundError","evalue":"No module named 'wandb.keras'","output_type":"error"}]},{"cell_type":"code","source":"# Set up TensorBoard callback\ntensorboard_callback = TensorBoard(log_dir='./logs', update_freq='epoch')","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:35:27.122194Z","iopub.status.idle":"2024-05-16T13:35:27.122622Z","shell.execute_reply.started":"2024-05-16T13:35:27.122409Z","shell.execute_reply":"2024-05-16T13:35:27.122426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:38:45.922882Z","iopub.execute_input":"2024-05-16T13:38:45.923609Z","iopub.status.idle":"2024-05-16T13:38:59.730225Z","shell.execute_reply.started":"2024-05-16T13:38:45.923568Z","shell.execute_reply":"2024-05-16T13:38:59.728735Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (8.1.2)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\nRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Tries to detect TPU\n    print(\"Running on TPU:\", tpu.master())\nexcept ValueError:\n    print(\"Could not connect to TPU\")\n    tpu = None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:38:59.732414Z","iopub.execute_input":"2024-05-16T13:38:59.732851Z","iopub.status.idle":"2024-05-16T13:38:59.740417Z","shell.execute_reply.started":"2024-05-16T13:38:59.732802Z","shell.execute_reply":"2024-05-16T13:38:59.739277Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Could not connect to TPU\n","output_type":"stream"}]},{"cell_type":"code","source":"if tpu:\n    try:\n        # Initialize the TPU system\n        print(\"Initializing TPU...\")\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"TPU initialized\")\n    except _:\n        print(\"Failed to initialize TPU\")\nelse:\n    print(\"Using default strategy (CPU/GPU)\")\n    strategy = tf.distribute.get_strategy()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:38:59.743037Z","iopub.execute_input":"2024-05-16T13:38:59.743760Z","iopub.status.idle":"2024-05-16T13:38:59.766256Z","shell.execute_reply.started":"2024-05-16T13:38:59.743725Z","shell.execute_reply":"2024-05-16T13:38:59.765267Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using default strategy (CPU/GPU)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"if tpu:\n    try:\n        # Initialize the TPU system\n        print(\"Initializing TPU...\")\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"TPU initialized\")\n    except _:\n        print(\"Failed to initialize TPU\")\nelse:\n    print(\"Using default strategy (CPU/GPU)\")\n    strategy = tf.distribute.get_strategy()\n","metadata":{}},{"cell_type":"code","source":"!pip install imageio","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:38:59.768358Z","iopub.execute_input":"2024-05-16T13:38:59.768718Z","iopub.status.idle":"2024-05-16T13:39:12.814692Z","shell.execute_reply.started":"2024-05-16T13:38:59.768685Z","shell.execute_reply":"2024-05-16T13:39:12.813544Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (2.33.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imageio) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio) (9.5.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:39:12.816057Z","iopub.execute_input":"2024-05-16T13:39:12.816468Z","iopub.status.idle":"2024-05-16T13:39:25.293125Z","shell.execute_reply.started":"2024-05-16T13:39:12.816423Z","shell.execute_reply":"2024-05-16T13:39:25.292067Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (8.1.2)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\nRequirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras import layers\n\nimport io\nimport imageio\nfrom IPython.display import Image, display\nfrom ipywidgets import widgets, Layout, HBox","metadata":{"id":"iyIrFKLalUjV","execution":{"iopub.status.busy":"2024-05-16T13:39:25.294681Z","iopub.execute_input":"2024-05-16T13:39:25.295028Z","iopub.status.idle":"2024-05-16T13:39:25.435923Z","shell.execute_reply.started":"2024-05-16T13:39:25.294996Z","shell.execute_reply":"2024-05-16T13:39:25.434848Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Construction\n\nFor this example, we will be using the\n[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\ndataset.\n\nWe will download the dataset and then construct and\npreprocess training and validation sets.\n\nFor next-frame prediction, our model will be using a previous frame,\nwhich we'll call `f_n`, to predict a new frame, called `f_(n + 1)`.\nTo allow the model to create these predictions, we'll need to process\nthe data such that we have \"shifted\" inputs and outputs, where the\ninput data is frame `x_n`, being used to predict frame `y_(n + 1)`.","metadata":{"id":"UiisescrL5pt"}},{"cell_type":"code","source":"# Downloading the Moving MNIST dataset\nfpath = keras.utils.get_file(\n    \"moving_mnist.npy\",\n    \"http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\",\n)\ndataset = np.load(fpath)\n\n# swapping the axes representing the number of frames and number of data samples\ndataset = np.swapaxes(dataset, 0, 1)\n# we pick out 1000 of the 10000 total exmaples and use those. using all of it\ndataset = dataset[:1000, ...]\n# adding a channel dimension since the images are grayscale.\ndataset = np.expand_dims(dataset, axis=-1)\n\n# splitting into train and validation sets using indexing to optimize memory.\nindexes = np.arange(dataset.shape[0])\nnp.random.shuffle(indexes)\ntrain_index = indexes[: int(0.9 * dataset.shape[0])]\nval_index = indexes[int(0.9 * dataset.shape[0]) :]\ntrain_dataset = dataset[train_index]\nval_dataset = dataset[val_index]\n\n# normalizing the data to the 0-1 range.\ntrain_dataset = train_dataset / 255\nval_dataset = val_dataset / 255\n\n# we define a helper function to shift the frames, where\n# 'x' is frame 0 to n - 1, and 'y' is frames 1 to n.\n'''\ndef create_shifted_frames(data):\n  x = data[:, 0 : data.shape[1] - 1, :, :]\n  y = data[:, 1 : data.shape[1], :, :]\n  return x,y\n'''\n#changed the function a bit\ndef create_shifted_frames(data):\n    # Correctly shift the frames to ensure `x` has one frame less than `y`\n    x = data[:, :-1, :, :, :]\n    y = data[:, 1:, :, :, :]\n    return x, y\n\n\n# apply the processing function to the datasets.\nx_train, y_train = create_shifted_frames(train_dataset)\nx_val, y_val = create_shifted_frames(val_dataset)\n\n#inspect the dataset.\nprint(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\nprint(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))\n\n","metadata":{"id":"RwVV9tb6mLWu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b99ffb1-29e2-4770-a8be-77f4a986a927","execution":{"iopub.status.busy":"2024-05-16T13:40:31.919198Z","iopub.execute_input":"2024-05-16T13:40:31.919615Z","iopub.status.idle":"2024-05-16T13:40:32.284586Z","shell.execute_reply.started":"2024-05-16T13:40:31.919582Z","shell.execute_reply":"2024-05-16T13:40:32.283364Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Downloading the Moving MNIST dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fpath \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoving_mnist.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(fpath)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# swapping the axes representing the number of frames and number of data samples\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"],"ename":"NameError","evalue":"name 'keras' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef create_tf_dataset(x, y, batch_size=8, cache=False):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    if cache:\n        dataset = dataset.cache()\n    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset\n\n# Create TensorFlow datasets for training and validation\ntrain_tf_dataset = create_tf_dataset(x_train, y_train, batch_size=32, cache=True)\nval_tf_dataset = create_tf_dataset(x_val, y_val, batch_size=32, cache=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T13:39:43.002975Z","iopub.execute_input":"2024-05-16T13:39:43.003810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_tf_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization\n\nOur data consists of sequences of frames, each of which\nare used to predict the upcoming frame. Let's take a look\nat some of these sequential frames.","metadata":{"id":"uX7kTs3ySN88"}},{"cell_type":"code","source":"# Contruct a figure on which we will visualize the images\nfig, axes = plt.subplots(4, 5, figsize=(10, 8))\n\n# Plot each of the sequential images for one random data example.\ndata_choice = np.random.choice(range(len(train_dataset)), size = 1)[0]\nfor idx, ax in enumerate(axes.flat):\n    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n    ax.set_title(f\"Frame {idx + 1}\")\n    ax.axis(\"off\")\n\n# Print information and display the figure.\nprint(f\"Displaying frames for example {data_choice}.\")\nplt.show()\n","metadata":{"id":"Rs3LsDeUmT2Y","colab":{"base_uri":"https://localhost:8080/","height":692},"outputId":"b8a348f7-8ab8-435f-c8b4-d93555101e45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Construction\n\nTo build a Convolutional LSTM model, we will use the\n`ConvLSTM2D` layer, which will accept inputs of shape\n`(batch_size, num_frames, width, height, channels)`, and return\na prediction movie of the same shape.\n\n","metadata":{"id":"G7NQedSrUZca"}},{"cell_type":"markdown","source":"custom_loss that takes the true and predicted values for both frames and optical flow. The function calculates the frame loss using the Huber loss and the optical flow loss using the Huber loss as well. The total loss is the sum of the frame loss and the optical flow loss, weighted by a hyperparameter lambda","metadata":{}},{"cell_type":"code","source":"'''\nwith strategy.scope():\n    # Constructing the input layer with no definite frame size.\n    inp = tf.keras.layers.Input(shape=(None, *x_train.shape[2:]))\n\n    # Constructing 5 `ConvLSTM2D` layers with batch normalization,\n    # followed by a `Conv3D` layer for the spatiotemporal outputs.\n    x = tf.keras.layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(5, 5),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(inp)\n    x = tf.keras.layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = tf.keras.layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = tf.keras.layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(1, 1),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = tf.keras.layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(1, 1),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = tf.keras.layers.Conv3D(\n        filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n    )(x)\n\n    # Next, we will build the complete model and compile it.\n    model = tf.keras.models.Model(inp, x)\n    model.compile(\n        loss=tf.keras.losses.MAE(),\n        optimizer=tf.keras.optimizers.Adam(),\n    )\n    '''\n\n#making a vae implementation","metadata":{"id":"FIU6g8dfsj5x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, callbacks\nfrom tensorflow.keras.optimizers import Adam\n\n# Custom sampling layer for VAE\nclass Sampling(layers.Layer):\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n# Define the VAE encoder\ndef build_encoder(input_shape, latent_dim):\n    encoder_inputs = layers.Input(shape=(None, *input_shape))\n    x = layers.ConvLSTM2D(\n        filters=32,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(encoder_inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=False,\n        activation=\"relu\",\n    )(x)\n    x = layers.Flatten()(x)\n    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n    z = Sampling()([z_mean, z_log_var])\n    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n    return encoder\n\n# Define the VAE decoder\ndef build_decoder(latent_dim, output_shape):\n    latent_inputs = layers.Input(shape=(latent_dim,))\n    x = layers.Dense(64 * 8 * 8 * 20, activation=\"relu\")(latent_inputs)\n    x = layers.Reshape((20, 8, 8, 64))(x)\n    x = layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ConvLSTM2D(\n        filters=64,\n        kernel_size=(3, 3),\n        padding=\"same\",\n        return_sequences=True,\n        activation=\"relu\",\n    )(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv3D(\n        filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n    )(x)\n    x = layers.TimeDistributed(layers.UpSampling2D(size=(8, 8)))(x)\n    decoder = Model(latent_inputs, x, name=\"decoder\")\n    return decoder\n\n# Define the VAE model\nclass VAE(Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def train_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(x)\n            reconstruction = self.decoder(z)\n            reconstruction_loss = tf.reduce_mean(\n                tf.keras.losses.binary_crossentropy(y, reconstruction)\n            )\n            reconstruction_loss *= 64 * 64 * 20\n            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n            kl_loss = tf.reduce_mean(kl_loss)\n            kl_loss *= -0.5\n            total_loss = reconstruction_loss + kl_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\n# Parameters\ninput_shape = x_train.shape[2:]\nlatent_dim = 2\noutput_shape = input_shape\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n\nWith our model and data constructed, we can now train the model.","metadata":{"id":"FUNqsvz8Y-Lp"}},{"cell_type":"code","source":"# Build encoder and decoder\nwith strategy.scope():\n    encoder = build_encoder(input_shape, latent_dim)\n    decoder = build_decoder(latent_dim, output_shape)\n\n    # Build and compile VAE model\n    vae = VAE(encoder, decoder)\n    vae.compile(optimizer=Adam(learning_rate=0.001))\n\n    # Define callbacks\n    early_stopping = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n    tensorboard_callback = callbacks.TensorBoard(log_dir='./logs')\n\n    # Training hyperparameters\n    epochs = 1000\n    batch_size = 128  # Start with 64, can be increased to 128, 256, etc.\n\n    # Fitting the model to the training data\n    vae.fit(\n        train_tf_dataset,\n        epochs=epochs,\n        validation_data=val_tf_dataset,\n        callbacks=[early_stopping, reduce_lr, tensorboard_callback],\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # Define some callbacks to improve training.\n    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n\n    # Define modificable training hyperparameters\n    epochs = 1000\n    batch_size = 128\n\n    # Fitting the model to the training data.\n    model.fit(\n        train_tf_dataset,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=val_tf_dataset,\n        callbacks=[early_stopping, reduce_lr, tensorboard_callback],\n)","metadata":{"id":"CJr4T9l3O1wg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"380bd180-30da-4080-a215-646bc601f7f4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir ./logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frame Prediction Visualizations\n\nWith our model now constructed and trained, we can generate\nsome example frame predictions based on a new video.\n\nWe'll pick a random example from the validation set and\nthen choose the first ten frames from them. From there, we can\nallow the model to predict 10 new frames, which we can compare\nto the ground truth frame predictions.","metadata":{"id":"A0JJPwxQZ4_m"}},{"cell_type":"code","source":"# Select a random example from the validation dataset.\nexample = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n\n# Pick the first/last ten frames from the example.\nframes = example[:10, ...]\noriginal_frames = example[10:, ...]\n\n# Predict a new set of 10 frames.\nfor _ in range(10):\n    # Extract the model's prediction and post-process it.\n    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n    new_prediction = np.squeeze(new_prediction, axis=0)\n    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n\n    # Extend the set of prediction frames.\n    frames = np.concatenate((frames, predicted_frame), axis=0)\n\n# Construct a figure for the original and new frames.\nfig, axes = plt.subplots(2, 10, figsize=(20, 4))\n\n# Plot the original frames.\nfor idx, ax in enumerate(axes[0]):\n    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")\n    ax.set_title(f\"Frame {idx + 11}\")\n    ax.axis(\"off\")\n\n# Plot the new frames.\nnew_frames = frames[10:, ...]\nfor idx, ax in enumerate(axes[1]):\n    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n    ax.set_title(f\"Frame {idx + 11}\")\n    ax.axis(\"off\")\n\n# Display the figure.\nplt.show()\n","metadata":{"id":"l_wCQqVMO3jf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicted Videos\n\nFinally, we'll pick a few examples from the validation set\nand construct some GIFs with them to see the model's\npredicted videos.\n\nWe can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/conv-lstm)\nand try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/conv-lstm)","metadata":{"id":"0aFxUhK7gZHv"}},{"cell_type":"code","source":"# Selecting a few random examples from the dataset.\nexamples = val_dataset[np.random.choice(range(len(val_dataset)), size=5)]\n\n# Iterate over the examples and predict the frames.\npredicted_videos = []\nfor example in examples:\n  # Pick the first/last ten frames from the example.\n  frames = example[10:, ...]\n  original_frames = example[10:, ...]\n  new_predictions = np.zeros(shape=(10, *frames[0].shape))\n\n  # Predict a new set of 10 frames.\n  for i in range(10):\n      # Extract the model's prediction and post-process it.\n      frames = example[: 10 + i + 1, ...]\n      new_prediction = model.predict(np.expand_dims(frames, axis=0))\n      new_prediction = np.squeeze(new_prediction, axis=0)\n      predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n\n      # Extending the set of prediction frames.\n      new_predictions[i] = predicted_frame\n\n  # Creating and saving GIFs for each of the ground truth/prediction images.\n  for frame_set in [original_frames, new_predictions]:\n      #Construct a GIF from the selected video frames.\n      current_frames = np.squeeze(frame_set)\n      current_frames = current_frames[..., np.newaxis] * np.ones(3)\n      current_frames = (current_frames * 255).astype(np.uint8)\n      current_frames = list(current_frames)\n\n      # Construct a GIF from the frames.\n      with io.BytesIO() as gif:\n          imageio.mimsave(gif, current_frames, \"GIF\", duration=200)\n          predicted_videos.append(gif.getvalue())\n\n# Display the videos.\nprint(\" Truth\\tPrediction\")\nfor i in range(0, len(predicted_videos), 2):\n  # Construct and display an 'HBox'  with the ground truth and prediction.\n  box = HBox(\n      [\n          widgets.Image(value=predicted_videos[i]),\n          widgets.Image(value=predicted_videos[i + 1]),\n      ]\n  )\n  display(box)\n\n","metadata":{"id":"DZTgO3koO8JA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/')\n","metadata":{"id":"zpMi64tD43Dz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"JAX implementation\n","metadata":{}},{"cell_type":"code","source":"!pip install stax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install flax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import flax\nimport flax.nn as nn\nimport jax\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\n\n# Load the Moving MNIST dataset\ndataset = np.load(\"moving_mnist.npy\")\ndataset = np.swapaxes(dataset, 0, 1)\ndataset = dataset[:1000, ...]\ndataset = np.expand_dims(dataset, axis=-1)\n\n# Split into training and validation sets\nindexes = np.arange(dataset.shape[0])\nnp.random.shuffle(indexes)\ntrain_index = indexes[: int(0.9 * dataset.shape[0])]\nval_index = indexes[int(0.9 * dataset.shape[0]) :]\ntrain_dataset = dataset[train_index]\nval_dataset = dataset[val_index]\n\n# Normalize the data to the 0-1 range\ntrain_dataset = train_dataset / 255\nval_dataset = val_dataset / 255\n\n# Define the helper function to shift the frames\ndef create_shifted_frames(data):\n    x = data[:, 0 : data.shape[1] - 1, :, :]\n    y = data[:, 1 : data.shape[1], :, :]\n    return x, y\n\n# Apply the processing function to the datasets\nx_train, y_train = create_shifted_frames(train_dataset)\nx_val, y_val = create_shifted_frames(val_dataset)\n\n# Define the Convolutional LSTM model\nclass ConvLSTM(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.ConvLSTM(num_features=64, kernel_shape=(5, 5), stride=1, padding=\"SAME\")(x)\n        x = nn.ConvLSTM(num_features=64, kernel_shape=(3, 3), stride=1, padding=\"SAME\")(x)\n        x = nn.ConvLSTM(num_features=64, kernel_shape=(3, 3), stride=1, padding=\"SAME\")(x)\n        x = nn.ConvLSTM(num_features=64, kernel_shape=(1, 1), stride=1, padding=\"SAME\")(x)\n        x = nn.ConvLSTM(num_features=64, kernel_shape=(1, 1), stride=1, padding=\"SAME\")(x)\n        x = nn.Conv(num_features=1, kernel_shape=(3, 3, 3), stride=1, padding=\"SAME\")(x)\n        return x\n\n# Initialize the model parameters\nmodel = ConvLSTM()\nparams = model.init(jax.random.PRNGKey(0), x_train)\n\n# Define the loss function and optimizer\ndef loss_fn(params, x, y):\n    y_pred = model.apply(params, x)\n    return jnp.mean((y_pred - y) ** 2)\n\nopt_init, opt_update, get_params = flax.optimizers.adam(1e-4)\nopt_state = opt_init(params)\n\n# Train the model\nfor epoch in range(1000):\n    for x_batch, y_batch in zip(x_train, y_train):\n        grads = jax.grad(loss_fn)(get_params(opt_state), x_batch, y_batch)\n        opt_state = opt_update(epoch, grads, opt_state)\n    print(f\"Epoch {epoch}, Loss: {loss_fn(get_params(opt_state), x_batch, y_batch)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}