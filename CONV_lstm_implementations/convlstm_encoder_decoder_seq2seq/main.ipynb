{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorBoard started at http://localhost:6006/ for version 0\n",
      "\n",
      "\n",
      "TensorBoard started at http://localhost:6006/ for version research\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1563c85fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                   | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model     | EncoderDecoderConvLSTM | 260 K  | train\n",
      "1 | criterion | MSELoss                | 0      | train\n",
      "-------------------------------------------------------------\n",
      "260 K     Trainable params\n",
      "0         Non-trainable params\n",
      "260 K     Total params\n",
      "1.040     Total estimated model params size (MB)\n",
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4459c311018b489e9f788f675f60783a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboard:Deleting accumulator '.'\n",
      "WARNING:tensorboard:Deleting accumulator '.'\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "import glob\n",
    "from tensorboard import program\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--beta_1', type=float, default=0.9, help='decay rate 1')\n",
    "    parser.add_argument('--beta_2', type=float, default=0.98, help='decay rate 2')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train for')\n",
    "    parser.add_argument('--use_amp', default=True, type=bool, help='mixed-precision training')\n",
    "    parser.add_argument('--n_gpus', type=int, default=2, help='number of GPUs')\n",
    "    parser.add_argument('--n_hidden_dim', type=int, default=32, help='number of hidden dim for ConvLSTM layers')\n",
    "    \n",
    "    args = parser.parse_args([])  # Parse empty list to use defaults\n",
    "    \n",
    "    # Override with your desired values\n",
    "    args.n_gpus = 1\n",
    "    args.use_amp = True\n",
    "    args.batch_size = 64\n",
    "    \n",
    "    return args\n",
    "\n",
    "opt = parse_args()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import socket\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# from: https://github.com/edenton/svg/blob/master/data/moving_mnist.py\n",
    "\n",
    "class MovingMNIST(object):\n",
    "    \"\"\"Data Handler that creates Bouncing MNIST dataset on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, train, data_root, seq_len=20, num_digits=2, image_size=64, deterministic=True):\n",
    "        path = data_root\n",
    "        self.seq_len = seq_len\n",
    "        self.num_digits = num_digits\n",
    "        self.image_size = image_size\n",
    "        self.step_length = 0.1\n",
    "        self.digit_size = 32\n",
    "        self.deterministic = deterministic\n",
    "        self.seed_is_set = False  # multi threaded loading\n",
    "        self.channels = 1\n",
    "\n",
    "        self.data = datasets.MNIST(\n",
    "            path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(self.digit_size),\n",
    "                 transforms.ToTensor()]))\n",
    "\n",
    "        self.N = len(self.data)\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if not self.seed_is_set:\n",
    "            self.seed_is_set = True\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.set_seed(index)\n",
    "        image_size = self.image_size\n",
    "        digit_size = self.digit_size\n",
    "        x = np.zeros((self.seq_len,\n",
    "                      image_size,\n",
    "                      image_size,\n",
    "                      self.channels),\n",
    "                     dtype=np.float32)\n",
    "        for n in range(self.num_digits):\n",
    "            idx = np.random.randint(self.N)\n",
    "            digit, _ = self.data[idx]\n",
    "\n",
    "            sx = np.random.randint(image_size - digit_size)\n",
    "            sy = np.random.randint(image_size - digit_size)\n",
    "            dx = np.random.randint(-4, 5)\n",
    "            dy = np.random.randint(-4, 5)\n",
    "            for t in range(self.seq_len):\n",
    "                if sy < 0:\n",
    "                    sy = 0\n",
    "                    if self.deterministic:\n",
    "                        dy = -dy\n",
    "                    else:\n",
    "                        dy = np.random.randint(1, 5)\n",
    "                        dx = np.random.randint(-4, 5)\n",
    "                elif sy >= image_size - 32:\n",
    "                    sy = image_size - 32 - 1\n",
    "                    if self.deterministic:\n",
    "                        dy = -dy\n",
    "                    else:\n",
    "                        dy = np.random.randint(-4, 0)\n",
    "                        dx = np.random.randint(-4, 5)\n",
    "\n",
    "                if sx < 0:\n",
    "                    sx = 0\n",
    "                    if self.deterministic:\n",
    "                        dx = -dx\n",
    "                    else:\n",
    "                        dx = np.random.randint(1, 5)\n",
    "                        dy = np.random.randint(-4, 5)\n",
    "                elif sx >= image_size - 32:\n",
    "                    sx = image_size - 32 - 1\n",
    "                    if self.deterministic:\n",
    "                        dx = -dx\n",
    "                    else:\n",
    "                        dx = np.random.randint(-4, 0)\n",
    "                        dy = np.random.randint(-4, 5)\n",
    "\n",
    "                x[t, sy:sy + 32, sx:sx + 32, 0] += digit.numpy().squeeze()\n",
    "                sy += dy\n",
    "                sx += dx\n",
    "\n",
    "        x[x > 1] = 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, nf, in_chan):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoder Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
    "        # Decoder (3D CNN) - produces regression predictions for our model\n",
    "\n",
    "        \"\"\"\n",
    "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf,  # nf + 1\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=nf,\n",
    "                                     out_channels=1,\n",
    "                                     kernel_size=(1, 3, 3),\n",
    "                                     padding=(0, 1, 1))\n",
    "\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # encoder\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :],\n",
    "                                               cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
    "            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n",
    "                                                 cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
    "\n",
    "        # encoder_vector\n",
    "        encoder_vector = h_t2\n",
    "\n",
    "        # decoder\n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n",
    "                                                 cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3,\n",
    "                                                 cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
    "            encoder_vector = h_t4\n",
    "            outputs += [h_t4]  # predictions\n",
    "\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, future_seq=0, hidden_state=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, channel, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "\n",
    "        # initialize hidden states\n",
    "        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        # autoencoder forward\n",
    "        outputs = self.autoencoder(x, seq_len, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "######### MODEL ##########\n",
    "##########################\n",
    "\n",
    "class MovingMNISTLightning(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams=None, model=None):\n",
    "        super(MovingMNISTLightning, self).__init__()\n",
    "\n",
    "        # default config\n",
    "        self.path = os.getcwd() + '/data'\n",
    "        self.model = model\n",
    "\n",
    "        # logging config\n",
    "        self.log_images = True\n",
    "\n",
    "        # Training config\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.batch_size = opt.batch_size\n",
    "        self.n_steps_past = 10\n",
    "        self.n_steps_ahead = 10  # 4\n",
    "\n",
    "\n",
    "    def create_video(self, x, y_hat, y):\n",
    "        # predictions with input for illustration purposes\n",
    "        preds = torch.cat([x.cpu(), y_hat.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        # entire input and ground truth\n",
    "        y_plot = torch.cat([x.cpu(), y.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        # error (l2 norm) plot between pred and ground truth\n",
    "        difference = (torch.pow(y_hat[0] - y[0], 2)).detach().cpu()\n",
    "        zeros = torch.zeros(difference.shape)\n",
    "        difference_plot = torch.cat([zeros.cpu().unsqueeze(0), difference.unsqueeze(0).cpu()], dim=1)[\n",
    "            0].unsqueeze(1)\n",
    "\n",
    "        # concat all images\n",
    "        final_image = torch.cat([preds, y_plot, difference_plot], dim=0)\n",
    "\n",
    "        # make them into a single grid image file\n",
    "        grid = torchvision.utils.make_grid(final_image, nrow=self.n_steps_past + self.n_steps_ahead)\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device='cuda')\n",
    "\n",
    "        output = self.model(x, future_seq=self.n_steps_ahead)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:, 0:self.n_steps_past, :, :, :], batch[:, self.n_steps_past:, :, :, :]\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        y = y.squeeze()\n",
    "\n",
    "        y_hat = self.forward(x).squeeze()  # is squeeze neccessary?\n",
    "\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        # save learning_rate\n",
    "        lr_saved = self.trainer.optimizers[0].param_groups[-1]['lr']\n",
    "        lr_saved = torch.scalar_tensor(lr_saved).cuda()\n",
    "\n",
    "        # save predicted images every 250 global_step\n",
    "        if self.log_images:\n",
    "            if self.global_step % 250 == 0:\n",
    "                final_image = self.create_video(x, y_hat, y)\n",
    "\n",
    "                self.logger.experiment.add_image(\n",
    "                    'epoch_' + str(self.current_epoch) + '_step' + str(self.global_step) + '_generated_images',\n",
    "                    final_image, 0)\n",
    "                plt.close()\n",
    "\n",
    "        tensorboard_logs = {'train_mse_loss': loss,\n",
    "                            'learning_rate': lr_saved}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'test_loss': self.criterion(y_hat, y)}\n",
    "\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=opt.lr, betas=(opt.beta_1, opt.beta_2))\n",
    "\n",
    "    #@pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        train_data = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=self.path,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            deterministic=True,\n",
    "            num_digits=2)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    #@pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test_data = MovingMNIST(\n",
    "            train=False,\n",
    "            data_root=self.path,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            deterministic=True,\n",
    "            num_digits=2)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "        return test_loader\n",
    "\n",
    "def run_tensorboard(new_run=False):\n",
    "    path = os.getcwd() + '/lightning_logs/'\n",
    "    try:\n",
    "        newest_folder = max(glob.glob(os.path.join(path, '*/')), key=os.path.getmtime)\n",
    "        version_number = newest_folder.split('/')[-2].split('_')[1]\n",
    "        if new_run:\n",
    "            new_version_number = str(int(version_number) + 1)\n",
    "            newest_folder = newest_folder.replace(version_number, new_version_number)\n",
    "            version_number = new_version_number # for print purposes\n",
    "    except ValueError:\n",
    "        version_number = 0\n",
    "        newest_folder = path + 'version_0'\n",
    "    \n",
    "    while not os.path.exists(newest_folder):\n",
    "        time.sleep(1)\n",
    "    \n",
    "    tb = program.TensorBoard()\n",
    "    tb.configure(argv=[None, '--logdir', newest_folder])\n",
    "    url = tb.launch()\n",
    "    print(f\"\\nTensorBoard started at {url} for version {version_number}\\n\")\n",
    "    return url\n",
    "\n",
    "def start_tensorboard_thread(new_run=False):\n",
    "    thread = threading.Thread(target=run_tensorboard, args=(new_run,))\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "# Start TensorBoard\n",
    "tensorboard_thread = start_tensorboard_thread(new_run=True)\n",
    "# Display TensorBoard in the notebook\n",
    "tensorboard_url = run_tensorboard(new_run=False)\n",
    "display(IFrame(src=tensorboard_url, width=800, height=600))\n",
    "\n",
    "def run_trainer():\n",
    "    conv_lstm_model = EncoderDecoderConvLSTM(nf=opt.n_hidden_dim, in_chan=1)\n",
    "\n",
    "    model = MovingMNISTLightning(model=conv_lstm_model)\n",
    "\n",
    "    trainer = Trainer(max_epochs=opt.epochs,\n",
    "                      devices=opt.n_gpus,\n",
    "                      accelerator=\"gpu\",\n",
    "                      #strategy='ddp',\n",
    "                      enable_checkpointing=False,\n",
    "                      precision=16 if opt.use_amp else 32\n",
    "                      )\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p1 = Process(target=run_trainer)                    # start trainer\n",
    "#     p1.start()\n",
    "#     p2 = Process(target=run_tensorboard(new_run=True))  # start tensorboard\n",
    "#     p2.start()\n",
    "#     p1.join()\n",
    "#     p2.join()\n",
    "\n",
    "\n",
    "run_trainer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
