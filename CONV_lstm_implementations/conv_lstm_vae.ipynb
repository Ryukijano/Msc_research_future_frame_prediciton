{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--beta_1', type=float, default=0.9, help='decay rate 1')\n",
    "    parser.add_argument('--beta_2', type=float, default=0.98, help='decay rate 2')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=300, help='number of epochs to train for')\n",
    "    parser.add_argument('--use_amp', default=True, type=bool, help='mixed-precision training')\n",
    "    parser.add_argument('--n_gpus', type=int, default=2, help='number of GPUs')\n",
    "    parser.add_argument('--n_hidden_dim', type=int, default=64, help='number of hidden dim for ConvLSTM layers')\n",
    "    \n",
    "    args = parser.parse_args([])  # Parse empty list to use defaults\n",
    "    \n",
    "    # Override with your desired values\n",
    "    args.n_gpus = 1\n",
    "    args.use_amp = True\n",
    "    args.batch_size = 128\n",
    "    \n",
    "    return args\n",
    "\n",
    "opt = parse_args()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import socket\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# from: https://github.com/edenton/svg/blob/master/data/moving_mnist.py\n",
    "\n",
    "class MovingMNIST(object):\n",
    "    \"\"\"Data Handler that creates Bouncing MNIST dataset on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, train, data_root, seq_len=20, num_digits=2, image_size=64, deterministic=True):\n",
    "        path = data_root\n",
    "        self.seq_len = seq_len\n",
    "        self.num_digits = num_digits\n",
    "        self.image_size = image_size\n",
    "        self.step_length = 0.1\n",
    "        self.digit_size = 32\n",
    "        self.deterministic = deterministic\n",
    "        self.seed_is_set = False  # multi threaded loading\n",
    "        self.channels = 1\n",
    "\n",
    "        self.data = datasets.MNIST(\n",
    "            path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(self.digit_size),\n",
    "                 transforms.ToTensor()]))\n",
    "\n",
    "        self.N = len(self.data)\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if not self.seed_is_set:\n",
    "            self.seed_is_set = True\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.set_seed(index)\n",
    "        image_size = self.image_size\n",
    "        digit_size = self.digit_size\n",
    "        x = np.zeros((self.seq_len,\n",
    "                      image_size,\n",
    "                      image_size,\n",
    "                      self.channels),\n",
    "                     dtype=np.float32)\n",
    "        for n in range(self.num_digits):\n",
    "            idx = np.random.randint(self.N)\n",
    "            digit, _ = self.data[idx]\n",
    "\n",
    "            sx = np.random.randint(image_size - digit_size)\n",
    "            sy = np.random.randint(image_size - digit_size)\n",
    "            dx = np.random.randint(-4, 5)\n",
    "            dy = np.random.randint(-4, 5)\n",
    "            for t in range(self.seq_len):\n",
    "                if sy < 0:\n",
    "                    sy = 0\n",
    "                    if self.deterministic:\n",
    "                        dy = -dy\n",
    "                    else:\n",
    "                        dy = np.random.randint(1, 5)\n",
    "                        dx = np.random.randint(-4, 5)\n",
    "                elif sy >= image_size - 32:\n",
    "                    sy = image_size - 32 - 1\n",
    "                    if self.deterministic:\n",
    "                        dy = -dy\n",
    "                    else:\n",
    "                        dy = np.random.randint(-4, 0)\n",
    "                        dx = np.random.randint(-4, 5)\n",
    "\n",
    "                if sx < 0:\n",
    "                    sx = 0\n",
    "                    if self.deterministic:\n",
    "                        dx = -dx\n",
    "                    else:\n",
    "                        dx = np.random.randint(1, 5)\n",
    "                        dy = np.random.randint(-4, 5)\n",
    "                elif sx >= image_size - 32:\n",
    "                    sx = image_size - 32 - 1\n",
    "                    if self.deterministic:\n",
    "                        dx = -dx\n",
    "                    else:\n",
    "                        dx = np.random.randint(-4, 0)\n",
    "                        dy = np.random.randint(-4, 5)\n",
    "\n",
    "                x[t, sy:sy + 32, sx:sx + 32, 0] += digit.numpy().squeeze()\n",
    "                sy += dy\n",
    "                sx += dx\n",
    "\n",
    "        x[x > 1] = 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class VAEConvLSTM(nn.Module):\n",
    "    def __init__(self, nf, in_chan):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoder Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
    "        # Decoder (3D CNN) - produces regression predictions for our model\n",
    "\n",
    "        \"\"\"\n",
    "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "        #VAE components\n",
    "        self.fc_mu = nn.Conv2d(nf, latent_dim, kernel_size=1)\n",
    "        self.fc_logvar = nn.Conv2d(nf, latent_dim, kernel_size = 1)\n",
    "\n",
    "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf,  # nf + 1\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=nf,\n",
    "                                     out_channels=1,\n",
    "                                     kernel_size=(1, 3, 3),\n",
    "                                     padding=(0, 1, 1))\n",
    "\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # encoder\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :],\n",
    "                                               cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
    "            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n",
    "                                                 cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
    "\n",
    "        # encoder_vector\n",
    "        encoder_vector = h_t2\n",
    "\n",
    "        # decoder\n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n",
    "                                                 cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3,\n",
    "                                                 cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
    "            encoder_vector = h_t4\n",
    "            outputs += [h_t4]  # predictions\n",
    "\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, future_seq=0, hidden_state=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, channel, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "\n",
    "        # initialize hidden states\n",
    "        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        # autoencoder forward\n",
    "        outputs = self.autoencoder(x, seq_len, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "######### MODEL ##########\n",
    "##########################\n",
    "\n",
    "class MovingMNISTVAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams=None, model=None):\n",
    "        super(MovingMNISTVAE, self).__init__()\n",
    "        self.vae_convlstm= VAEConvLSTM(nf, in_chan, latent_dim)\n",
    "\n",
    "        # default config\n",
    "        self.path = os.getcwd() + '/data'\n",
    "        self.model = model\n",
    "\n",
    "        '''CHECK IF YOU ACTUALLY NEED THIS\n",
    "        self.lr = lr\n",
    "        '''\n",
    "\n",
    "        # logging config\n",
    "        self.log_images = True\n",
    "\n",
    "        # Training config\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.batch_size = opt.batch_size\n",
    "        self.n_steps_past = 10\n",
    "        self.n_steps_ahead = 10  # 4\n",
    "\n",
    "    def create_video(self, x, y_hat, y):\n",
    "        # predictions with input for illustration purposes\n",
    "        preds = torch.cat([x.cpu(), y_hat.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        # entire input and ground truth\n",
    "        y_plot = torch.cat([x.cpu(), y.unsqueeze(2).cpu()], dim=1)[0]\n",
    "\n",
    "        # error (l2 norm) plot between pred and ground truth\n",
    "        difference = (torch.pow(y_hat[0] - y[0], 2)).detach().cpu()\n",
    "        zeros = torch.zeros(difference.shape)\n",
    "        difference_plot = torch.cat([zeros.cpu().unsqueeze(0), difference.unsqueeze(0).cpu()], dim=1)[\n",
    "            0].unsqueeze(1)\n",
    "\n",
    "        # concat all images\n",
    "        final_image = torch.cat([preds, y_plot, difference_plot], dim=0)\n",
    "\n",
    "        # make them into a single grid image file\n",
    "        grid = torchvision.utils.make_grid(final_image, nrow=self.n_steps_past + self.n_steps_ahead)\n",
    "\n",
    "        return grid\n",
    "    \n",
    "\n",
    "    # OPERATIONS IN FUNCTIONS ARE MOVED AROUND A TAD BIT, HOPEFULLY SHOULDN'T AFFECT ANYTHING\n",
    "    def forward(self, x):\n",
    "        x = x.to(device='cuda')\n",
    "        #permutted earlier because of the VAE architecture also, need to consider the probabilistic latent space now.\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        output, mu, logvar = self.vae_convlstm(x, self.n_steps_ahead)\n",
    "        return output, mu, logvar\n",
    "    \n",
    "    #NEW LOSS FUNCTION INTRODUCED FOR VAE\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        #Binary CROSS ENTROPY ONLY FOR THE MOVING MNIST DATASET BECAUSE OF 1  CHANNEL AND TWO PIXEL VALUES BLACK AND WHITE(EASY TO CLASSIFY)\n",
    "        #NEED TO IMPLEMENT MSE(MEAN SQUARED ERROR) or MAE(MEAN AVERAGE ERROR) FOR 3 CHANNEL RGB IMAGES.\n",
    "        BCE = F.binary_cross_entropy_with_logits(recon_x, x, reduction='sum')\n",
    "        #NEED THESE LOSS FUNCTIONS WHEN USING 3 CHANNEL RGB IMAGES\n",
    "        #MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        #MAE = F.l1_loss(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1+ logvar - mu.pow(2) - logvar.exp())\n",
    "        #the return would be changed when using MSE or MAE\n",
    "        return BCE + KLD\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:, 0:self.n_steps_past, :, :, :], batch[:, self.n_steps_past:, :, :, :]\n",
    "        #MOVING around things a bit \n",
    "        #x = x.permute(0, 1, 4, 2, 3)\n",
    "    \n",
    "        y = y.squeeze()\n",
    "        \n",
    "        '''\n",
    "        The active selection of code is calling the `loss_function` method within a class. This method is used to calculate the loss for a variational autoencoder (VAE) model. The `loss_function` takes four parameters: `recon_x`, `x`, `mu`, and `logvar`. \n",
    "\n",
    "        In the provided implementation of the `loss_function`, the loss is calculated using a combination of binary cross entropy (BCE) and the Kullback-Leibler Divergence (KLD). \n",
    "\n",
    "        First, the BCE is calculated using the `F.binary_cross_entropy_with_logits` function. This function computes the binary cross entropy loss between the reconstructed output (`recon_x`) and the target input (`x`). The `reduction='sum'` argument specifies that the loss should be summed over all elements in the input tensors.\n",
    "\n",
    "        Next, the KLD is computed using the formula `-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())`. The KLD measures the difference between the learned distribution of the latent space variables (`mu` and `logvar`) and a prior distribution (usually a standard normal distribution). It penalizes the model for deviating from the prior distribution. The KLD term encourages the VAE to learn a compact and smooth latent space representation.\n",
    "\n",
    "        Finally, the BCE and KLD terms are added together to obtain the total loss. The return statement of the `loss_function` method returns the sum of the BCE and KLD terms.\n",
    "\n",
    "        It's worth noting that the implementation of the `loss_function` in the provided code is specifically designed for the Moving MNIST dataset, which has a single channel and two pixel values (black and white). For RGB images with three channels, different loss functions such as mean squared error (MSE) or mean absolute error (MAE) would be more appropriate.\n",
    "        '''\n",
    "        y_hat, mu, logvar = self(x)\n",
    "        loss = self.loss_function(y_hat, y, mu, logvar)\n",
    "        return loss\n",
    "\n",
    "        #few things changed around here since the loss function is different now and its vae \n",
    "        #y_hat = self.forward(x).squeeze()  # is squeeze neccessary?\n",
    "\n",
    "        #loss = self.criterion(y_hat, y)\n",
    "\n",
    "        # save learning_rate\n",
    "        lr_saved = self.trainer.optimizers[0].param_groups[-1]['lr']\n",
    "        lr_saved = torch.scalar_tensor(lr_saved).cuda()\n",
    "\n",
    "        # save predicted images every 250 global_step\n",
    "        if self.log_images:\n",
    "            if self.global_step % 250 == 0:\n",
    "                final_image = self.create_video(x, y_hat, y)\n",
    "\n",
    "                self.logger.experiment.add_image(\n",
    "                    'epoch_' + str(self.current_epoch) + '_step' + str(self.global_step) + '_generated_images',\n",
    "                    final_image, 0)\n",
    "                plt.close()\n",
    "\n",
    "        tensorboard_logs = {'train_mse_loss': loss,\n",
    "                            'learning_rate': lr_saved}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'test_loss': self.criterion(y_hat, y)}\n",
    "\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=opt.lr, betas=(opt.beta_1, opt.beta_2))\n",
    "        #THE ONE BELOW IS SUGGESTED BY PERPLEXITY.\n",
    "        #return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    #@pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        train_data = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=self.path,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            deterministic=True,\n",
    "            num_digits=2)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    #@pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test_data = MovingMNIST(\n",
    "            train=False,\n",
    "            data_root=self.path,\n",
    "            seq_len=self.n_steps_past + self.n_steps_ahead,\n",
    "            image_size=64,\n",
    "            deterministic=True,\n",
    "            num_digits=2)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "        return test_loader\n",
    "\n",
    "\n",
    "\n",
    "def run_trainer():\n",
    "    conv_lstm_model = EncoderDecoderConvLSTM(nf=opt.n_hidden_dim, in_chan=1)\n",
    "\n",
    "    model = MovingMNISTLightning(model=conv_lstm_model)\n",
    "\n",
    "    trainer = Trainer(max_epochs=opt.epochs,\n",
    "                      devices=opt.n_gpus,\n",
    "                      accelerator=\"gpu\",\n",
    "                      #strategy='ddp',\n",
    "                      enable_checkpointing=False,\n",
    "                      precision=16 if opt.use_amp else 32\n",
    "                      )\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p1 = Process(target=run_trainer)                    # start trainer\n",
    "#     p1.start()\n",
    "#     p2 = Process(target=run_tensorboard(new_run=True))  # start tensorboard\n",
    "#     p2.start()\n",
    "#     p1.join()\n",
    "#     p2.join()\n",
    "\n",
    "\n",
    "run_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
