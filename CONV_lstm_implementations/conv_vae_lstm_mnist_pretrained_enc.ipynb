{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (1.13)\n",
      "Requirement already satisfied: wandb in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.6.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "import easydict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Wandb API key from Kaggle Secrets\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.login()  # This will now use the API key from the environment variable\n",
    "wandb.init(project=\"convlstmvae-moving-mnist\", entity=\"ryukijano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.login()  # This will now use the API key from the environment variable\n",
    "wandb.init(project=\"convlstmvae-moving-mnist\", entity=\"ryukijano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "#loading a pre-trained ResNet and modifying it\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity() #removing the final fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size=4096, hidden_size=1024, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return (hidden, cell)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=4096, hidden_size=1024, output_size=4096, num_layers=2\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        output, (hidden, cell) = self.lstm(x, hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, (hidden, cell)\n",
    "\n",
    "\n",
    "class LSTMVAE(nn.Module):\n",
    "    \"\"\"LSTM-based Variational Auto Encoder\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, latent_size, device=torch.device(\"cuda\")\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_size: int, batch_size x sequence_length x input_dim\n",
    "        hidden_size: int, output size of LSTM AE\n",
    "        latent_size: int, latent z-layer size\n",
    "        num_lstm_layer: int, number of layers in LSTM\n",
    "        \"\"\"\n",
    "        super(LSTMVAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layers = 1\n",
    "\n",
    "        # lstm ae\n",
    "        self.lstm_enc = Encoder(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=self.num_layers\n",
    "        )\n",
    "        self.lstm_dec = Decoder(\n",
    "            input_size=latent_size,\n",
    "            output_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc21 = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc22 = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc3 = nn.Linear(self.latent_size, self.hidden_size)\n",
    "\n",
    "    def reparametize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(std).to(self.device)\n",
    "\n",
    "        z = mu + noise * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # encode input space to hidden space\n",
    "        enc_hidden = self.lstm_enc(x)\n",
    "        enc_h = enc_hidden[0].view(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        enc_c = enc_hidden[1].view(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "\n",
    "        # extract latent variable z(hidden space to latent space)\n",
    "        mean = self.fc21(enc_h[-1])\n",
    "        logvar = self.fc22(enc_h[-1])\n",
    "        z = self.reparametize(mean, logvar)  # batch_size x latent_size\n",
    "\n",
    "        # initialize hidden state as inputs\n",
    "        h_ = self.fc3(z).view(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_ = torch.zeros_like(h_)\n",
    "        \n",
    "        # decode latent space to input space\n",
    "        z = z.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        z = z.view(batch_size, seq_len, self.latent_size).to(self.device)\n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden = (h_.contiguous(), c_.contiguous())\n",
    "        reconstruct_output, hidden = self.lstm_dec(z, hidden)\n",
    "\n",
    "        x_hat = reconstruct_output\n",
    "\n",
    "        # calculate vae loss\n",
    "        losses = self.loss_function(x_hat, x, mean, logvar)\n",
    "        m_loss, recon_loss, kld_loss = losses[\"loss\"], losses[\"Reconstruction_Loss\"], losses[\"KLD\"]\n",
    "\n",
    "        return m_loss, x_hat, (recon_loss, kld_loss)\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = 0.00025  # Account for the minibatch samples from the dataset\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"Reconstruction_Loss\": recons_loss.detach(),\n",
    "            \"KLD\": -kld_loss.detach(),\n",
    "        }\n",
    "\n",
    "\n",
    "class LSTMAE(nn.Module):\n",
    "    \"\"\"LSTM-based Auto Encoder\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, latent_size, device=torch.device(\"cuda\")):\n",
    "        \"\"\"\n",
    "        input_size: int, batch_size x sequence_length x input_dim\n",
    "        hidden_size: int, output size of LSTM AE\n",
    "        latent_size: int, latent z-layer size\n",
    "        num_lstm_layer: int, number of layers in LSTM\n",
    "        \"\"\"\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # lstm ae\n",
    "        self.lstm_enc = Encoder(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "        self.lstm_dec = Decoder(\n",
    "            input_size=input_size,\n",
    "            output_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        enc_hidden = self.lstm_enc(x)\n",
    "\n",
    "        temp_input = torch.zeros((batch_size, seq_len, feature_dim), dtype=torch.float).to(\n",
    "            self.device\n",
    "        )\n",
    "        hidden = enc_hidden\n",
    "        reconstruct_output, hidden = self.lstm_dec(temp_input, hidden)\n",
    "        reconstruct_loss = self.criterion(reconstruct_output, x)\n",
    "\n",
    "        return reconstruct_loss, reconstruct_output, (0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_channels + self.hidden_channels,\n",
    "            out_channels=4 * self.hidden_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMEncoder, self).__init__()\n",
    "        self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        h = torch.zeros(batch_size, self.convlstm.hidden_channels, height, width).to(x.device)\n",
    "        c = torch.zeros(batch_size, self.convlstm.hidden_channe7ls, height, width).to(x.device)\n",
    "        7\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.convlstm(x[:, t, :, :, :], h, c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "class ConvLSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, kernel_size):\n",
    "        super(ConvLSTMDecoder, self).__init__()\n",
    "        self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "        self.conv_out = nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, h, c, seq_len):\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(seq_len):\n",
    "            h, c = self.convlstm(x, h, c)\n",
    "            output = self.conv_out(h)\n",
    "            outputs.append(output)\n",
    "            x = output\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "#Defining CONVLSTMVAE with a Pre-trained Encoder\n",
    "class CONVLSTMVAE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, latent_size, kernel_size=3):\n",
    "        super(CONVLSTMVAE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder = ConvLSTMEncoder(input_channels, hidden_channels, kernel_size)\n",
    "        self.decoder = ConvLSTMDecoder(input_channels, hidden_channels, input_channels, kernel_size)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_channels * 64 * 64, latent_size)  # Assuming 64x64 spatial dimensions\n",
    "        self.fc_logvar = nn.Linear(hidden_channels * 64 * 64, latent_size)\n",
    "        self.fc_decode = nn.Linear(latent_size, hidden_channels * 64 * 64)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        \n",
    "        # Encode\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            cnn_output = self.encoder(x[:, t, :, :, :])\n",
    "            cnn_features.append(cnn_output)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "\n",
    "        h,c = self.convlstm_encoder(cnn_features)\n",
    "        h_flat = h.reshape(batch_size, -1)\n",
    "        \n",
    "        # VAE bottleneck\n",
    "        mu = self.fc_mu(h_flat)\n",
    "        logvar = self.fc_logvar(h_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode\n",
    "        h_decoded = self.fc_decode(z).view(batch_size, -1, height, width)\n",
    "        c_decoded = torch.zeros_like(h_decoded)\n",
    "        x_decoded = torch.zeros(batch_size, channels, height, width).to(x.device)\n",
    "        \n",
    "        output = self.decoder(x_decoded, h_decoded, c_decoded, seq_len)\n",
    "        \n",
    "        return output, mu, logvar\n",
    "    \n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "\n",
    "# Visualization function\n",
    "def visualize_reconstructions(model, val_loader, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            recon_x, _, _ = model(x)\n",
    "            break\n",
    "    x = x.cpu().numpy()\n",
    "    recon_x = recon_x.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(x[i, 0, 0], cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(recon_x[i, 0, 0], cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.suptitle(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n",
    "    plt.savefig(f\"reconstruction_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, val_loader, optimizer, epochs, device):\n",
    "    scaler = GradScaler()\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                recon_x, mu, logvar = model(x)\n",
    "                loss = model.loss_function(recon_x, x, mu, logvar)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_reconstructions(model, val_loader, device, epoch)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    train_dataset = MovingMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    val_dataset = MovingMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        seqs, targets = zip(*batch)  # Separating sequences and targets\n",
    "        seqs = torch.stack(seqs).unsqueeze(2).permute(0, 1, 2, 4, 3)  # Adjust dimensions\n",
    "        targets = torch.stack(targets).unsqueeze(2).permute(0, 1, 2, 4, 3)  # Adjust dimensions\n",
    "        return seqs, targets\n",
    "\n",
    "# Example usage\n",
    "input_channels = 1  # 3 For RGB videos\n",
    "hidden_channels = 64\n",
    "latent_size = 128\n",
    "model = CONVLSTMVAE(input_channels, hidden_channels, latent_size)\n",
    "\n",
    "# # Assuming input shape: (batch_size, sequence_length, channels, height, width)\n",
    "# sample_input = torch.randn(16, 10, 3, 64, 64)\n",
    "# output, mu, logvar = model(sample_input)\n",
    "\n",
    "# print(f\"Input shape: {sample_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Mu shape: {mu.shape}\")\n",
    "# print(f\"Logvar shape: {logvar.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
