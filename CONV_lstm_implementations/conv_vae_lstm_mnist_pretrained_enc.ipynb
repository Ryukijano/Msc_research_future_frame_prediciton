{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (1.13)\n",
      "Requirement already satisfied: wandb in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.6.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\sc23gd\\appdata\\local\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "import easydict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Wandb API key from Kaggle Secrets\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.login()  # This will now use the API key from the environment variable\n",
    "wandb.init(project=\"convlstmvae-moving-mnist\", entity=\"ryukijano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "#loading a pre-trained ResNet and modifying it\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity() #removing the final fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import codecs\n",
    "import errno\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MovingMNIST(data.Dataset):\n",
    "    \"\"\"`MovingMNIST <http://www.cs.toronto.edu/~nitish/unsupervised_video/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``processed/training.pt``\n",
    "            and  ``processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        split (int, optional): Train/test split size. Number defines how many samples\n",
    "            belong to test set.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in an PIL\n",
    "            image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "    \"\"\"\n",
    "\n",
    "    urls = [\"https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz\"]\n",
    "    raw_folder = \"raw\"\n",
    "    processed_folder = \"processed\"\n",
    "    training_file = \"moving_mnist_train.pt\"\n",
    "    test_file = \"moving_mnist_test.pt\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        train=True,\n",
    "        split=1000,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "    ):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.split = split\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found.\" + \" You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file)\n",
    "            )\n",
    "        else:\n",
    "            self.test_data = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (seq, target) where sampled sequences are splitted into a seq\n",
    "                    and target part\n",
    "        \"\"\"\n",
    "\n",
    "        # need to iterate over time\n",
    "        def _transform_time(data):\n",
    "            new_data = None\n",
    "            for i in range(data.size(0)):\n",
    "                img = Image.fromarray(data[i].numpy(), mode=\"L\")\n",
    "                new_data = (\n",
    "                    self.transform(img)\n",
    "                    if new_data is None\n",
    "                    else torch.cat([self.transform(img), new_data], dim=0)\n",
    "                )\n",
    "            return new_data\n",
    "\n",
    "        if self.train:\n",
    "            seq, target = self.train_data[index, :10], self.train_data[index, 10:]\n",
    "        else:\n",
    "            seq, target = self.test_data[index, :10], self.test_data[index, 10:]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            seq = _transform_time(seq)\n",
    "        if self.target_transform is not None:\n",
    "            target = _transform_time(target)\n",
    "\n",
    "        return seq, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(\n",
    "            os.path.join(self.root, self.processed_folder, self.training_file)\n",
    "        ) and os.path.exists(\n",
    "            os.path.join(self.root, self.processed_folder, self.test_file)\n",
    "        )\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the Moving MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import gzip\n",
    "\n",
    "        from six.moves import urllib\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for url in self.urls:\n",
    "            print(\"Downloading \" + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition(\"/\")[2]\n",
    "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(data.read())\n",
    "            with open(file_path.replace(\".gz\", \"\"), \"wb\") as out_f, gzip.GzipFile(\n",
    "                file_path\n",
    "            ) as zip_f:\n",
    "                out_f.write(zip_f.read())\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print(\"Processing...\")\n",
    "\n",
    "        training_set = torch.from_numpy(\n",
    "            np.load(\n",
    "                os.path.join(self.root, self.raw_folder, \"mnist_test_seq.npy\")\n",
    "            ).swapaxes(0, 1)[: -self.split]\n",
    "        )\n",
    "        test_set = torch.from_numpy(\n",
    "            np.load(\n",
    "                os.path.join(self.root, self.raw_folder, \"mnist_test_seq.npy\")\n",
    "            ).swapaxes(0, 1)[-self.split :]\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.root, self.processed_folder, self.training_file), \"wb\"\n",
    "        ) as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(\n",
    "            os.path.join(self.root, self.processed_folder, self.test_file), \"wb\"\n",
    "        ) as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = \"Dataset \" + self.__class__.__name__ + \"\\n\"\n",
    "        fmt_str += \"    Number of datapoints: {}\\n\".format(self.__len__())\n",
    "        tmp = \"train\" if self.train is True else \"test\"\n",
    "        fmt_str += \"    Train/test: {}\\n\".format(tmp)\n",
    "        fmt_str += \"    Root Location: {}\\n\".format(self.root)\n",
    "        tmp = \"    Transforms (if any): \"\n",
    "        fmt_str += \"{0}{1}\\n\".format(\n",
    "            tmp, self.transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n",
    "        )\n",
    "        tmp = \"    Target Transforms (if any): \"\n",
    "        fmt_str += \"{0}{1}\".format(\n",
    "            tmp, self.target_transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n",
    "        )\n",
    "        return fmt_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.input_channels + self.hidden_channels,\n",
    "            out_channels=4 * self.hidden_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMEncoder, self).__init__()\n",
    "        self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        h = torch.zeros(batch_size, self.convlstm.hidden_channels, height, width).to(x.device)\n",
    "        c = torch.zeros(batch_size, self.convlstm.hidden_channe7ls, height, width).to(x.device)\n",
    "        7\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.convlstm(x[:, t, :, :, :], h, c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "class ConvLSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, kernel_size):\n",
    "        super(ConvLSTMDecoder, self).__init__()\n",
    "        self.convlstm = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "        self.conv_out = nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, h, c, seq_len):\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(seq_len):\n",
    "            h, c = self.convlstm(x, h, c)\n",
    "            output = self.conv_out(h)\n",
    "            outputs.append(output)\n",
    "            x = output\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "#Defining CONVLSTMVAE with a Pre-trained Encoder\n",
    "class CONVLSTMVAE(nn.Module):\n",
    "    def __init__(self, encoder, input_channels, hidden_channels, latent_size, kernel_size=3):\n",
    "        super(CONVLSTMVAE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder = ConvLSTMEncoder(input_channels, hidden_channels, kernel_size)\n",
    "        self.decoder = ConvLSTMDecoder(input_channels, hidden_channels, 1, kernel_size)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_channels * 64 * 64, latent_size)  # Assuming 64x64 spatial dimensions\n",
    "        self.fc_logvar = nn.Linear(hidden_channels * 64 * 64, latent_size)\n",
    "        self.fc_decode = nn.Linear(latent_size, hidden_channels * 64 * 64)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        \n",
    "        # Encode\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            cnn_output = self.encoder(x[:, t, :, :, :])\n",
    "            cnn_features.append(cnn_output)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "\n",
    "        h,c = self.convlstm_encoder(cnn_features)\n",
    "        h_flat = h.reshape(batch_size, -1)\n",
    "        \n",
    "        # VAE bottleneck\n",
    "        mu = self.fc_mu(h_flat)\n",
    "        logvar = self.fc_logvar(h_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode\n",
    "        h_decoded = self.fc_decode(z).view(batch_size, -1, height, width)\n",
    "        c_decoded = torch.zeros_like(h_decoded)\n",
    "        x_decoded = torch.zeros(batch_size, channels, height, width).to(x.device)\n",
    "        \n",
    "        output = self.decoder(x_decoded, h_decoded, c_decoded, seq_len)\n",
    "        \n",
    "        return output, mu, logvar\n",
    "    \n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "\n",
    "# # Visualization function\n",
    "# def visualize_reconstructions(model, val_loader, device, epoch):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             x, _ = batch\n",
    "#             x = x.to(device)\n",
    "#             recon_x, _, _ = model(x)\n",
    "#             break\n",
    "#     x = x.cpu().numpy()\n",
    "#     recon_x = recon_x.cpu().numpy()\n",
    "\n",
    "#     fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "#     for i in range(10):\n",
    "#         axes[0, i].imshow(x[i, 0, 0], cmap='gray')\n",
    "#         axes[0, i].axis('off')\n",
    "#         axes[1, i].imshow(recon_x[i, 0, 0], cmap='gray')\n",
    "#         axes[1, i].axis('off')\n",
    "#     plt.suptitle(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n",
    "#     plt.savefig(f\"reconstruction_epoch_{epoch}.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# # Training function\n",
    "# def train(model, train_loader, val_loader, optimizer, epochs, device):\n",
    "#     scaler = GradScaler()\n",
    "#     model.to(device)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for batch in train_loader:\n",
    "#             x, _ = batch\n",
    "#             x = x.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             with autocast():\n",
    "#                 recon_x, mu, logvar = model(x)\n",
    "#                 loss = model.loss_function(recon_x, x, mu, logvar)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         avg_loss = total_loss / len(train_loader.dataset)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             visualize_reconstructions(model, val_loader, device, epoch)\n",
    "\n",
    "# # Main function\n",
    "# def main():\n",
    "#     # Data preparation\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "#     ])\n",
    "#     train_dataset = MovingMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "#     val_dataset = MovingMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    \n",
    "#     def collate_fn(batch):\n",
    "#         seqs, targets = zip(*batch)  # Separating sequences and targets\n",
    "#         seqs = torch.stack(seqs).unsqueeze(2).permute(0, 1, 2, 4, 3)  # Adjust dimensions\n",
    "#         targets = torch.stack(targets).unsqueeze(2).permute(0, 1, 2, 4, 3)  # Adjust dimensions\n",
    "#         return seqs, targets\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Example usage\n",
    "input_channels = 1  # 3 For RGB videos\n",
    "hidden_channels = 64\n",
    "latent_size = 128\n",
    "kernel_size = 3\n",
    "model = CONVLSTMVAE(resnet, input_channels, hidden_channels, latent_size)\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming input shape: (batch_size, sequence_length, channels, height, width)\n",
    "# sample_input = torch.randn(16, 10, 3, 64, 64)\n",
    "# output, mu, logvar = model(sample_input)\n",
    "\n",
    "# print(f\"Input shape: {sample_input.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Mu shape: {mu.shape}\")\n",
    "# print(f\"Logvar shape: {logvar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/71 [00:00<?, ?it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 238\u001b[0m\n\u001b[0;32m    222\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    223\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mtest_set,\n\u001b[0;32m    224\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    225\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m#     # convert to format of data loader\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m#     train_loader = torch.utils.data.DataLoader(\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m#         dataset=train_set, batch_size=args.batch_size, shuffle=True\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     id_ \u001b[38;5;241m=\u001b[39m secrets\u001b[38;5;241m.\u001b[39mtoken_hex(nbytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 101\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, train_loader, test_loader)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#casting operations to mixed precision\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 101\u001b[0m     mloss, recon_x, info \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#scaling the loss for better gradient flow\u001b[39;00m\n\u001b[0;32m    104\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(mloss\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 92\u001b[0m, in \u001b[0;36mCONVLSTMVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m---> 92\u001b[0m     cnn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     cnn_features\u001b[38;5;241m.\u001b[39mappend(cnn_output)\n\u001b[0;32m     94\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(cnn_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sc23gd\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 42\u001b[0m, in \u001b[0;36mConvLSTMEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 42\u001b[0m     batch_size, seq_len, channels, height, width \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     43\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvlstm\u001b[38;5;241m.\u001b[39mhidden_channels, height, width)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     44\u001b[0m     c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvlstm\u001b[38;5;241m.\u001b[39mhidden_channe7ls, height, width)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import secrets\n",
    "\n",
    "import easydict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "## visualization\n",
    "def imshow(past_data, title=\"MovingMNIST\"):\n",
    "    num_img = len(past_data)\n",
    "    fig = plt.figure(figsize=(4 * num_img, 4))\n",
    "\n",
    "    for idx in range(1, num_img + 1):\n",
    "        ax = fig.add_subplot(1, num_img + 1, idx)\n",
    "        ax.imshow(past_data[idx - 1])\n",
    "    plt.suptitle(title, fontsize=30)\n",
    "    plt.savefig(f\"{title}\")\n",
    "    plt.close()\n",
    "\n",
    "def visualize_reconstructions(model, test_loader, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(test_loader):\n",
    "            future_data, past_data = batch_data\n",
    "            batch_size = past_data.size(0)\n",
    "            example_size = past_data.size(1)\n",
    "            image_size = past_data.size(2), past_data.size(3)\n",
    "            #past_data = past_data.view(batch_size, example_size, -1).float().to(device)\n",
    "            #reshape for convvaelstm\n",
    "            past_data = past_data.reshape(batch_size, example_size, -1).float().to(device)\n",
    "\n",
    "            \n",
    "            _, recon_x, _ = model(past_data)\n",
    "            \n",
    "            if i == 0:\n",
    "                n_examples = min(10, batch_size)\n",
    "                #examples = past_data[:n_examples].cpu().view(n_examples, example_size, image_size[0], -1)\n",
    "                #reshape for convvaelstm\n",
    "                examples = past_data[:n_examples].cpu().reshape(n_examples, example_size, image_size[0], -1)\n",
    "                #recon_examples = recon_x[:n_examples].cpu().view(n_examples, example_size, image_size[0], -1)\n",
    "                recon_examples = recon_x[:n_examples].cpu().reshape(n_examples, example_size, image_size[0], -1)\n",
    "\n",
    "                fig, axes = plt.subplots(2, n_examples, figsize=(20, 4))\n",
    "                for j in range(n_examples):\n",
    "                    axes[0, j].imshow(examples[j, 0], cmap='gray')\n",
    "                    axes[0, j].axis('off')\n",
    "                    axes[1, j].imshow(recon_examples[j, 0], cmap='gray')\n",
    "                    axes[1, j].axis('off')\n",
    "                plt.suptitle(f\"Epoch {epoch}: Original (top) vs Reconstructed (bottom)\")\n",
    "                plt.savefig(f\"reconstruction_epoch_{epoch}.png\")\n",
    "                wandb.log({\"reconstructions\": wandb.Image(plt)})\n",
    "                plt.close()\n",
    "                break\n",
    "\n",
    "def train(args, model, train_loader, test_loader):\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    #creating a GradScaler for automatic mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    ## interation setup\n",
    "    epochs = tqdm(range(args.max_iter // len(train_loader) + 1))\n",
    "\n",
    "    ## training\n",
    "    count = 0\n",
    "    for epoch in epochs:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_iterator = tqdm(\n",
    "            enumerate(train_loader), total=len(train_loader), desc=\"training\"\n",
    "        )\n",
    "\n",
    "        for i, batch_data in train_iterator:\n",
    "\n",
    "            if count > args.max_iter:\n",
    "                return model\n",
    "            count += 1\n",
    "\n",
    "            future_data, past_data = batch_data\n",
    "            \n",
    "            '''\n",
    "            NO RESHAPE REQUIRED FOR INTO 2D for the CONVLSTM\n",
    "            ## reshape\n",
    "            batch_size = past_data.size(0)\n",
    "            example_size = past_data.size(1)\n",
    "            image_size = past_data.size(2), past_data.size(3)\n",
    "            past_data = (\n",
    "                past_data.view(batch_size, example_size, -1).float().to(args.device)\n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            past_data = past_data.float().to(args.device).half()\n",
    "\n",
    "            #casting operations to mixed precision\n",
    "            with autocast():\n",
    "                mloss, recon_x, info = model(past_data)\n",
    "            \n",
    "            #scaling the loss for better gradient flow\n",
    "            scaler.scale(mloss.mean()).backward()\n",
    "            \n",
    "            #unscales the gradients before optimization\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            #Update the GradScaler state\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "            train_iterator.set_postfix({\"train_loss\": float(mloss.mean())})\n",
    "        writer.add_scalar(\"train_loss\", float(mloss.mean()), epoch)\n",
    "        wandb.log({\"train_loss\": float(mloss.mean())})\n",
    "        \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        test_iterator = tqdm(\n",
    "            enumerate(test_loader), total=len(test_loader), desc=\"testing\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in test_iterator:\n",
    "                future_data, past_data = batch_data\n",
    "                '''No RESHAPE REQUIRED FOR CONVLSTMVAE\n",
    "                ## reshape\n",
    "                batch_size = past_data.size(0)\n",
    "                example_size = past_data.size(1)\n",
    "                past_data = (\n",
    "                    past_data.view(batch_size, example_size, -1).float().to(args.device)\n",
    "                )\n",
    "                '''\n",
    "                past_data = past_data.float().to(args.device).half()\n",
    "\n",
    "                #using autocast for validation\n",
    "                with autocast():\n",
    "                    mloss, recon_x, info = model(past_data)\n",
    "\n",
    "                eval_loss += mloss.mean().item()\n",
    "\n",
    "                test_iterator.set_postfix({\"eval_loss\": float(mloss.mean())})\n",
    "\n",
    "        eval_loss = eval_loss / len(test_loader)\n",
    "        writer.add_scalar(\"eval_loss\", float(eval_loss), epoch)\n",
    "        wandb.log({\"eval_loss\": float(eval_loss)})\n",
    "        print(\"Evaluation Score : [{}]\".format(eval_loss))\n",
    "\n",
    "        # Visualize reconstructions every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            visualize_reconstructions(model, test_loader, args.device, epoch)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # training dataset\n",
    "    train_set = MovingMNIST(\n",
    "        root=\".data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        target_transform=transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    # test dataset\n",
    "    test_set = MovingMNIST(\n",
    "        root=\".data/mnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        target_transform=transforms.ToTensor(),\n",
    "    )\n",
    "\n",
    "    args = easydict.EasyDict(\n",
    "        {\n",
    "            \"batch_size\": 128,\n",
    "            \"device\": torch.device(\"cuda\")\n",
    "            if torch.cuda.is_available()\n",
    "            else torch.device(\"cpu\"),\n",
    "            \"input_size\": 4096,\n",
    "            \"hidden_size\": 2048,\n",
    "            \"latent_size\": 1024,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"max_iter\": 1000,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    input_size = args.input_size\n",
    "    hidden_size = args.hidden_size\n",
    "    latent_size = args.latent_size\n",
    "\n",
    "    # define LSTM-based VAE model\n",
    "    #model = LSTMVAE(input_size, hidden_size, latent_size, device=args.device)\n",
    "    #model.to(args.device)\n",
    "    input_channels = 1 #since grayscale\n",
    "    hidden_channels = 64\n",
    "    latent_size = 128\n",
    "    kernel_size = 3\n",
    "    model = CONVLSTMVAE(input_channels, hidden_channels, latent_size, kernel_size)\n",
    "    model.to(args.device)\n",
    "    \n",
    "\n",
    "    '''Change required for CONVLSTMVAE'''\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        seqs, targets = zip(*batch) #Separating sequences and targets\n",
    "        seqs = torch.stack(seqs).unsqueeze(2).permute(0,1,2,4,3) #Adjust dimensions for sequences. Unsqueeze(2) for CONVLSTM to add an extra dimension for the channel before reordering the dimensions with the permute function.\n",
    "        targets = torch.stack(targets).unsqueeze(2).permute(0,1,2,4,3) #Adjust dimensions for sequences. Unsqueeze(2) for CONVLSTM to add an extra dimension for the channel before reordering the dimensions with the permute function.\n",
    "        return seqs, targets\n",
    "    \n",
    "    #convert to format of data loader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "#     # convert to format of data loader\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         dataset=train_set, batch_size=args.batch_size, shuffle=True\n",
    "#     )\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         dataset=test_set, batch_size=args.batch_size, shuffle=False\n",
    "#     )\n",
    "\n",
    "    # training\n",
    "    trained_model = train(args, model, train_loader, test_loader)\n",
    "\n",
    "    # save model\n",
    "    id_ = secrets.token_hex(nbytes=4)\n",
    "    model_path = f\"lstmvae{id_}.model\"\n",
    "    torch.save(trained_model.state_dict(), f\"lstmvae{id_}.model\")\n",
    "    wandb.save(model_path)\n",
    "\n",
    "    # load model\n",
    "    model_to_load = LSTMVAE(input_size, hidden_size, latent_size, device=args.device)\n",
    "    model_to_load.to(args.device)\n",
    "    model_to_load.load_state_dict(torch.load(f\"lstmvae{id_}.model\"))\n",
    "    model_to_load.eval()\n",
    "\n",
    "    # show results\n",
    "    ## past_data, future_data -> shape: (10,10)\n",
    "    future_data, past_data = train_set[0]\n",
    "\n",
    "    ## reshape\n",
    "    example_size = past_data.size(0)\n",
    "    image_size = past_data.size(1), past_data.size(2)\n",
    "    past_data = past_data.view(example_size, -1).float().to(args.device)\n",
    "    _, recon_data, info = model_to_load(past_data.unsqueeze(0))\n",
    "\n",
    "    nhw_orig = past_data.view(example_size, image_size[0], -1).cpu()\n",
    "    nhw_recon = (\n",
    "        recon_data.squeeze(0)\n",
    "        .view(example_size, image_size[0], -1)\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    imshow(nhw_orig, title=f\"final_input{id_}\")\n",
    "    imshow(nhw_recon, title=f\"final_output{id_}\")\n",
    "    plt.show()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
